<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
     畅院士的开山大弟子的博客
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/css/main.css">

  
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
  
  

  

</head>

</html>

<body>
  <div id="app">
    <main class="content">
      
<section class="cover">
    
      
      <a class="forkMe" href="https://github.com/Shen-Yu/hexo-theme-ayer"
        target="_blank"><img width="149" height="149" src="/images/forkme.png"
          class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/cover1.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">畅院士的开山大弟子的博客</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>

<div id="main">
  <section class="outer">
  <article class="articles">
    
    
    
    
    <article id="post-tensorflow2-0系列-六-：灵活建立模型" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/19/tensorflow2-0%E7%B3%BB%E5%88%97-%E5%85%AD-%EF%BC%9A%E7%81%B5%E6%B4%BB%E5%BB%BA%E7%AB%8B%E6%A8%A1%E5%9E%8B/"
    >tensorflow2.0系列(六)：灵活建立模型</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/19/tensorflow2-0%E7%B3%BB%E5%88%97-%E5%85%AD-%EF%BC%9A%E7%81%B5%E6%B4%BB%E5%BB%BA%E7%AB%8B%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time datetime="2020-03-19T06:32:30.000Z" itemprop="datePublished">2020-03-19</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>两种非继承类建立模型的方法：<br>sequential和functional<br>sequential：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model &#x3D; tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(100, activation&#x3D;tf.nn.relu),</span><br><span class="line">    tf.keras.layers.Dense(10),</span><br><span class="line">    tf.keras.layers.Softmax()</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<p>functional：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">inputs &#x3D; tf.keras.Input(shape&#x3D;(28, 28, 1))</span><br><span class="line">x &#x3D; tf.keras.layers.Flatten()(inputs)</span><br><span class="line">x &#x3D; tf.keras.layers.Dense(units&#x3D;100, activation&#x3D;tf.nn.relu)(x)</span><br><span class="line">x &#x3D; tf.keras.layers.Dense(units&#x3D;10)(x)</span><br><span class="line">outputs &#x3D; tf.keras.layers.Softmax()(x)</span><br><span class="line">model &#x3D; tf.keras.Model(inputs&#x3D;inputs, outputs&#x3D;outputs)</span><br></pre></td></tr></table></figure>

<p>通过这两种方式建立的模型要通过tf.keras.Model的compile方法配置训练过程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer&#x3D;tf.keras.optimizers.Adam(learning_rate&#x3D;0.001),</span><br><span class="line">    loss&#x3D;tf.keras.losses.sparse_categorical_crossentropy,</span><br><span class="line">    metrics&#x3D;[tf.keras.metrics.sparse_categorical_accuracy]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>tf.keras.Model.compile接受3个重要的参数：<br>oplimizer：优化器，从tf.keras.optimizers中选择；loss：损失函数，从tf.keras.losses中选择；metrics：评估指标，从tf.keras.metrics中选择。</p>
<p>接下来，使用tf.keras.Model的fit方法训练模型：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(data_loader.train_data, data_loader.train_label, epochs&#x3D;num_epochs, batch_size&#x3D;batch_size)</span><br></pre></td></tr></table></figure>
<p>tf.keras.Model.fit接受5个重要的参数：x：训练数据；y：目标数据(数据标签)；epochs：将训练数据迭代多少遍；batch_size：批次的大小；validation_data：验证数据，可用于在训练过程中监控模型的性能。<br>最后，使用tf.keras.Model.evaluate评估训练效果，提供测试数据及标签即可：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(model.evaluate(data_loader.test_data, data_loader.test_label))</span><br></pre></td></tr></table></figure>

<p>自定义层：不会</p>
<p>自定义损失函数和评估指标：<br>自定义损失函数需要继承tf.keras.losses.Loss类，重写call方法即可，传入真实值y_true和模型预测值 y_pred，输出模型预测值和真实值之间通过自定义的损失函数计算出的损失值。<br>下面的示例为均方差损失函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class MeanSquaredError(tf.keras.losses.Loss):</span><br><span class="line">    def call(self, y_true, y_pred):</span><br><span class="line">        return tf.reduce_mean(tf.square(y_pred - y_true))</span><br></pre></td></tr></table></figure>

<p>自定义评估指标：不会</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-北师大图论系列-一" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/19/%E5%8C%97%E5%B8%88%E5%A4%A7%E5%9B%BE%E8%AE%BA%E7%B3%BB%E5%88%97-%E4%B8%80/"
    >北师大图论系列(一)</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/19/%E5%8C%97%E5%B8%88%E5%A4%A7%E5%9B%BE%E8%AE%BA%E7%B3%BB%E5%88%97-%E4%B8%80/" class="article-date">
  <time datetime="2020-03-19T03:30:05.000Z" itemprop="datePublished">2020-03-19</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p><img src="/images/tulun/p1.jpg" alt=""></p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tulun/" rel="tag">tulun</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-tensorflow2-0系列-五-：卷积神经网络" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/18/tensorflow2-0%E7%B3%BB%E5%88%97-%E4%BA%94-%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"
    >tensorflow2.0系列(五)：卷积神经网络</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/18/tensorflow2-0%E7%B3%BB%E5%88%97-%E4%BA%94-%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="article-date">
  <time datetime="2020-03-18T11:53:59.000Z" itemprop="datePublished">2020-03-18</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>卷积神经网络的搭建：<br>卷积神经网络实际上是对大脑视觉皮层的神经元进行的建模，灵感来自于视觉皮层神经元每次只与前一层某个区域内神经元相连的现象。<br>代码示范：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">class CNN(tf.keras.Model):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv1 &#x3D; tf.keras.layers.Conv2D(</span><br><span class="line">            filters&#x3D;32,             # 卷积层神经元（卷积核）数目</span><br><span class="line">            kernel_size&#x3D;[5, 5],     # 感受野大小</span><br><span class="line">            padding&#x3D;&#39;same&#39;,         # padding策略（vaild 或 same）</span><br><span class="line">            activation&#x3D;tf.nn.relu   # 激活函数</span><br><span class="line">        )</span><br><span class="line">        self.pool1 &#x3D; tf.keras.layers.MaxPool2D(pool_size&#x3D;[2, 2], strides&#x3D;2)</span><br><span class="line">        self.conv2 &#x3D; tf.keras.layers.Conv2D(</span><br><span class="line">            filters&#x3D;64,</span><br><span class="line">            kernel_size&#x3D;[5, 5],</span><br><span class="line">            padding&#x3D;&#39;same&#39;,</span><br><span class="line">            activation&#x3D;tf.nn.relu</span><br><span class="line">        )</span><br><span class="line">        self.pool2 &#x3D; tf.keras.layers.MaxPool2D(pool_size&#x3D;[2, 2], strides&#x3D;2)</span><br><span class="line">        self.flatten &#x3D; tf.keras.layers.Reshape(target_shape&#x3D;(7 * 7 * 64,))</span><br><span class="line">        self.dense1 &#x3D; tf.keras.layers.Dense(units&#x3D;1024, activation&#x3D;tf.nn.relu)</span><br><span class="line">        self.dense2 &#x3D; tf.keras.layers.Dense(units&#x3D;10)</span><br><span class="line"></span><br><span class="line">    def call(self, inputs):</span><br><span class="line">        x &#x3D; self.conv1(inputs)                  # [batch_size, 28, 28, 32]</span><br><span class="line">        x &#x3D; self.pool1(x)                       # [batch_size, 14, 14, 32]</span><br><span class="line">        x &#x3D; self.conv2(x)                       # [batch_size, 14, 14, 64]</span><br><span class="line">        x &#x3D; self.pool2(x)                       # [batch_size, 7, 7, 64]</span><br><span class="line">        x &#x3D; self.flatten(x)                     # [batch_size, 7 * 7 * 64]</span><br><span class="line">        x &#x3D; self.dense1(x)                      # [batch_size, 1024]</span><br><span class="line">        x &#x3D; self.dense2(x)                      # [batch_size, 10]</span><br><span class="line">        output &#x3D; tf.nn.softmax(x)</span><br><span class="line">        return output</span><br></pre></td></tr></table></figure>
<p>感受野大小该怎么去理解。[3]<br>将上一节的model = MLP()换成 model = CNN()，可以发现准确率得到了显著的提高。</p>
<p>tf.keras.applications 中有一些预定义好的经典卷积神经网络结构，比如VGG16、VGG19 、ResNet、MobileNet等。我们可以直接调用这些经典的卷积神经网络结构（甚至载入预训练的参数），而无需手动定义网络结构。<br>比如，我们可以使用下列代码实例化一个MobileNetV2网络：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model &#x3D; tf.keras.applications.MobileNetV2()</span><br></pre></td></tr></table></figure>
<p>这行代码就相当于自己按照MobileNetV2的网络结构手写了一个类并且实例化:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class MobileNetV2():</span><br><span class="line">	pass</span><br><span class="line">model &#x3D; MobileNetV2()</span><br></pre></td></tr></table></figure>
<p>每个网络结构具有自己特定的详细参数设置,一些共通的常用参数如下：<br>input_shape：输入张量的形状(不含第一维的Batch)，大多默认为224×224×3。一般而言，模型对输入张量的大小有下限，长和宽至少为32×32或75×75。include_top:在网络的最后是否包含全连接层，默认为True；weights ：预训练权值，默认为’imagenet’，即为当前模型载入在ImageNet数据集上预训练的权值。如需随机初始化变量可设为None；classes：分类数，默认为1000。修改该参数需要include_top参数为True且weights参数为None。<br>下面展示一个例子，使用 MobileNetV2网络在tf_flowers五分类数据集上训练。通过将 weights设置为None，我们随机初始化变量而不使用预训练权值。同时将classes设置为5，对应于5分类的数据集。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow_datasets as tfds</span><br><span class="line"></span><br><span class="line">num_batches &#x3D; 1000</span><br><span class="line">batch_size &#x3D; 50</span><br><span class="line">learning_rate &#x3D; 0.001</span><br><span class="line"></span><br><span class="line">dataset &#x3D; tfds.load(&quot;tf_flowers&quot;, split&#x3D;tfds.Split.TRAIN, as_supervised&#x3D;True)</span><br><span class="line">dataset &#x3D; dataset.map(lambda img, label: (tf.image.resize(img, [224, 224]) &#x2F; 255.0, label)).shuffle(1024).batch(32)</span><br><span class="line">model &#x3D; tf.keras.applications.MobileNetV2(weights&#x3D;None, classes&#x3D;5)</span><br><span class="line">optimizer &#x3D; tf.keras.optimizers.Adam(learning_rate&#x3D;learning_rate)</span><br><span class="line">for images, labels in dataset:</span><br><span class="line">    with tf.GradientTape() as tape:</span><br><span class="line">        labels_pred &#x3D; model(images)</span><br><span class="line">        loss &#x3D; tf.keras.losses.sparse_categorical_crossentropy(y_true&#x3D;labels, y_pred&#x3D;labels_pred)</span><br><span class="line">        loss &#x3D; tf.reduce_mean(loss)</span><br><span class="line">        print(&quot;loss %f&quot; % loss.numpy())</span><br><span class="line">    grads &#x3D; tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(grads_and_vars&#x3D;zip(grads, model.trainable_variables))</span><br></pre></td></tr></table></figure>
<p>有一个小的区别是model.variables与model.trainable_variables。<br>加载数据和预处理数据的那两行代码没看懂。[1]</p>
<p>用tensorflow验证手推的卷积层运算：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># TensorFlow 的图像表示为 [图像数目，长，宽，色彩通道数] 的四维张量</span><br><span class="line"># 这里我们的输入图像 image 的张量形状为 [1, 7, 7, 1]</span><br><span class="line">image &#x3D; np.array([[</span><br><span class="line">    [0, 0, 0, 0, 0, 0, 0],</span><br><span class="line">    [0, 1, 0, 1, 2, 1, 0],</span><br><span class="line">    [0, 0, 2, 2, 0, 1, 0],</span><br><span class="line">    [0, 1, 1, 0, 2, 1, 0],</span><br><span class="line">    [0, 0, 2, 1, 1, 0, 0],</span><br><span class="line">    [0, 2, 1, 1, 2, 0, 0],</span><br><span class="line">    [0, 0, 0, 0, 0, 0, 0]</span><br><span class="line">]], dtype&#x3D;np.float32)</span><br><span class="line">image &#x3D; np.expand_dims(image, axis&#x3D;-1)  </span><br><span class="line">W &#x3D; np.array([[</span><br><span class="line">    [ 0, 0, -1], </span><br><span class="line">    [ 0, 1, 0 ], </span><br><span class="line">    [-2, 0, 2 ]</span><br><span class="line">]], dtype&#x3D;np.float32)</span><br><span class="line">b &#x3D; np.array([1], dtype&#x3D;np.float32)</span><br><span class="line"></span><br><span class="line">model &#x3D; tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Conv2D(</span><br><span class="line">        filters&#x3D;1,              # 卷积层神经元（卷积核）数目</span><br><span class="line">        kernel_size&#x3D;[3, 3],     # 感受野大小</span><br><span class="line">        kernel_initializer&#x3D;tf.constant_initializer(W),</span><br><span class="line">        bias_initializer&#x3D;tf.constant_initializer(b)</span><br><span class="line">    )]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">output &#x3D; model(image)</span><br><span class="line">print(tf.squeeze(output))</span><br></pre></td></tr></table></figure>
<p>tf.squeeze()方法的作用很有意思<br>有一个疑惑的地方就是一个四维的张量通过卷积层的输出是什么。[2]</p>
<p>如果图像是彩色的(即三通道)，这时我们就要为每个通道准备一个3×3的权值矩阵，即一共有3×3×3=27个权值。对于每个通道，均使用自己的权值矩阵进行处理，输出时将多个通道所输出的值进行加和。<br>在上面的代码中，每次卷积的结果都会丢掉一部分大小，有时这会为后面的工作带来麻烦。我们可以通过将tf.keras.layers.Conv2D中的padding参数设为same，从而使输出的矩阵大小和输入一致。<br>我们使用的是滑动窗口的方式进行卷积，所以可以设置每次滑动的步长。通过tf.keras.layers.Conv2D的strides参数即可设置步长（默认为1）。池化层现在一般用的都是max pooling，average pooling已经很少用了。</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-Andrew课后作业系列（一）" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/17/Andrew%E8%AF%BE%E5%90%8E%E4%BD%9C%E4%B8%9A%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89/"
    >Andrew课后作业系列（一）</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/17/Andrew%E8%AF%BE%E5%90%8E%E4%BD%9C%E4%B8%9A%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89/" class="article-date">
  <time datetime="2020-03-17T12:46:07.000Z" itemprop="datePublished">2020-03-17</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>用python实现hello world</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test &#x3D; &quot;Hello World&quot;</span><br><span class="line">print (&quot;test: &quot; + test)</span><br></pre></td></tr></table></figure>

<p>用math实现sigmoid函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import math</span><br><span class="line">def basic_sigmoid(x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Compute sigmoid of x.</span><br><span class="line">    Arguments:</span><br><span class="line">    x -- A scalar</span><br><span class="line">    Return:</span><br><span class="line">    s -- sigmoid(x)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    s &#x3D; 1&#x2F;(1+math.exp(-x))</span><br><span class="line">    return s</span><br></pre></td></tr></table></figure>

<p>basic_sigmoid()函数测试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">### One reason why we use &quot;numpy&quot; instead of &quot;math&quot; in Deep Learning ###</span><br><span class="line">x &#x3D; [1, 2, 3]</span><br><span class="line">basic_sigmoid(x) # you will see this give an error when you run it, because x is a vector.</span><br></pre></td></tr></table></figure>

<p>使用numpy而不是math库的一个很重要的原因就是math库不能传入一个向量，而numpy可以。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"># example of np.exp</span><br><span class="line">x &#x3D; np.array([1, 2, 3])</span><br><span class="line">print(np.exp(x)) # result is (exp(1), exp(2), exp(3))</span><br></pre></td></tr></table></figure>

<p>在python中，向量同样可以与标量相加，因为python的广播机制</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># example of vector operation</span><br><span class="line">x &#x3D; np.array([1, 2, 3])</span><br><span class="line">print (x + 3)</span><br></pre></td></tr></table></figure>

<p>用numpy实现sigmoid函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: sigmoid</span><br><span class="line">import numpy as np # this means you can access numpy functions by writing np.function() instead of numpy.function()</span><br><span class="line">def sigmoid(x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Compute the sigmoid of x</span><br><span class="line">    Arguments:</span><br><span class="line">    x -- A scalar or numpy array of any size</span><br><span class="line">    Return:</span><br><span class="line">    s -- sigmoid(x)</span><br><span class="line">    &quot;&quot;&quot; </span><br><span class="line">    s &#x3D; 1&#x2F;(1+np.exp(-x)) </span><br><span class="line">    return s</span><br></pre></td></tr></table></figure>

<p>测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; np.array([1, 2, 3])</span><br><span class="line">sigmoid(x)</span><br></pre></td></tr></table></figure>

<p>用numpy实现sigmoid的导数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def sigmoid_derivative(x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.</span><br><span class="line">    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.</span><br><span class="line">    Arguments:</span><br><span class="line">    x -- A scalar or numpy array</span><br><span class="line">    Return:</span><br><span class="line">    ds -- Your computed gradient.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    s &#x3D; 1&#x2F;(1+np.exp(-x))</span><br><span class="line">    ds &#x3D; s*(1-s)    </span><br><span class="line">    return ds</span><br></pre></td></tr></table></figure>

<p>测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; np.array([1, 2, 3])</span><br><span class="line">print (&quot;sigmoid_derivative(x) &#x3D; &quot; + str(sigmoid_derivative(x)))</span><br></pre></td></tr></table></figure>

<p>shape()和reshape()的使用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def image2vector(image):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Argument:</span><br><span class="line">    image -- a numpy array of shape (length, height, depth)</span><br><span class="line">    Returns:</span><br><span class="line">    v -- a vector of shape (length*height*depth, 1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    v &#x3D; image.reshape(image.shape[0]*image.shape[1],image.shape[2])</span><br><span class="line">    return v</span><br></pre></td></tr></table></figure>

<p>测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values</span><br><span class="line">image &#x3D; np.array([[[ 0.67826139,  0.29380381],</span><br><span class="line">        [ 0.90714982,  0.52835647],</span><br><span class="line">        [ 0.4215251 ,  0.45017551]],</span><br><span class="line"></span><br><span class="line">       [[ 0.92814219,  0.96677647],</span><br><span class="line">        [ 0.85304703,  0.52351845],</span><br><span class="line">        [ 0.19981397,  0.27417313]],</span><br><span class="line"></span><br><span class="line">       [[ 0.60659855,  0.00533165],</span><br><span class="line">        [ 0.10820313,  0.49978937],</span><br><span class="line">        [ 0.34144279,  0.94630077]]])</span><br><span class="line"></span><br><span class="line">print (&quot;image2vector(image) &#x3D; &quot; + str(image2vector(image)))</span><br></pre></td></tr></table></figure>

<p>按行归一化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def normalizeRows(x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement a function that normalizes each row of the matrix x (to have unit length). </span><br><span class="line">    Argument:</span><br><span class="line">    x -- A numpy matrix of shape (n, m)</span><br><span class="line">    Returns:</span><br><span class="line">    x -- The normalized (by row) numpy matrix. You are allowed to modify x.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord &#x3D; 2, axis &#x3D; ..., keepdims &#x3D; True)</span><br><span class="line">    x_norm &#x3D; np.linalg.norm(x,ord&#x3D;2,axis&#x3D;1,keepdims&#x3D;True) </span><br><span class="line">    # Divide x by its norm.</span><br><span class="line">    x &#x3D; x&#x2F;x_norm</span><br><span class="line">    return x</span><br></pre></td></tr></table></figure>
<p>np.linalg.norm()的使用还要查一下。[1]</p>
<p>测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; np.array([</span><br><span class="line">    [0, 3, 4],</span><br><span class="line">    [1, 6, 4]])</span><br><span class="line">print(&quot;normalizeRows(x) &#x3D; &quot; + str(normalizeRows(x)))</span><br></pre></td></tr></table></figure>

<p>定义softmax函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def softmax(x):</span><br><span class="line">    &quot;&quot;&quot;Calculates the softmax for each row of the input x.</span><br><span class="line">    Your code should work for a row vector and also for matrices of shape (n, m).</span><br><span class="line">    Argument:</span><br><span class="line">    x -- A numpy matrix of shape (n,m)</span><br><span class="line">    Returns:</span><br><span class="line">    s -- A numpy matrix equal to the softmax of x, of shape (n,m)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # Apply exp() element-wise to x. Use np.exp(...).</span><br><span class="line">    x_exp &#x3D; np.exp(x)</span><br><span class="line">    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis &#x3D; 1, keepdims &#x3D; True).</span><br><span class="line">    x_sum &#x3D; np.sum(x_exp,axis&#x3D;1,keepdims&#x3D;True)</span><br><span class="line">    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.</span><br><span class="line">    s &#x3D; x_exp&#x2F;x_sum</span><br><span class="line">    return s</span><br></pre></td></tr></table></figure>
<p>np.sum()中的axis和keepdim两个参数是啥意思？[2]</p>
<p>测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; np.array([</span><br><span class="line">    [9, 2, 5, 0, 0],</span><br><span class="line">    [7, 5, 0, 0 ,0]])</span><br><span class="line">print(&quot;softmax(x) &#x3D; &quot; + str(softmax(x)))</span><br></pre></td></tr></table></figure>

<p>向量化计算与for循环在时间上的比较：<br>用for循环实现向量之间的运算</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">x1 &#x3D; [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]</span><br><span class="line">x2 &#x3D; [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]</span><br><span class="line">### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###</span><br><span class="line">tic &#x3D; time.process_time()</span><br><span class="line">dot &#x3D; 0</span><br><span class="line">for i in range(len(x1)):</span><br><span class="line">    dot+&#x3D; x1[i]*x2[i]</span><br><span class="line">toc &#x3D; time.process_time()</span><br><span class="line">print (&quot;dot &#x3D; &quot; + str(dot) + &quot;\n ----- Computation time &#x3D; &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br><span class="line">### CLASSIC OUTER PRODUCT IMPLEMENTATION ###</span><br><span class="line">tic &#x3D; time.process_time()</span><br><span class="line">outer &#x3D; np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros</span><br><span class="line">for i in range(len(x1)):</span><br><span class="line">    for j in range(len(x2)):</span><br><span class="line">        outer[i,j] &#x3D; x1[i]*x2[j]</span><br><span class="line">toc &#x3D; time.process_time()</span><br><span class="line">print (&quot;outer &#x3D; &quot; + str(outer) + &quot;\n ----- Computation time &#x3D; &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br><span class="line">### CLASSIC ELEMENTWISE IMPLEMENTATION ###</span><br><span class="line">tic &#x3D; time.process_time()</span><br><span class="line">mul &#x3D; np.zeros(len(x1))</span><br><span class="line">for i in range(len(x1)):</span><br><span class="line">    mul[i] &#x3D; x1[i]*x2[i]</span><br><span class="line">toc &#x3D; time.process_time()</span><br><span class="line">print (&quot;elementwise multiplication &#x3D; &quot; + str(mul) + &quot;\n ----- Computation time &#x3D; &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br><span class="line">### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###</span><br><span class="line">W &#x3D; np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array</span><br><span class="line">tic &#x3D; time.process_time()</span><br><span class="line">gdot &#x3D; np.zeros(W.shape[0])</span><br><span class="line">for i in range(W.shape[0]):</span><br><span class="line">    for j in range(len(x1)):</span><br><span class="line">        gdot[i] +&#x3D; W[i,j]*x1[j]</span><br><span class="line">toc &#x3D; time.process_time()</span><br><span class="line">print (&quot;gdot &#x3D; &quot; + str(gdot) + &quot;\n ----- Computation time &#x3D; &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br></pre></td></tr></table></figure>

<p>用numpy库直接进行向量计算：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">x1 &#x3D; [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]</span><br><span class="line">x2 &#x3D; [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]</span><br><span class="line">### VECTORIZED DOT PRODUCT OF VECTORS ###</span><br><span class="line">tic &#x3D; time.process_time()</span><br><span class="line">dot &#x3D; np.dot(x1,x2)</span><br><span class="line">toc &#x3D; time.process_time()</span><br><span class="line">print (&quot;dot &#x3D; &quot; + str(dot) + &quot;\n ----- Computation time &#x3D; &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br><span class="line">### VECTORIZED OUTER PRODUCT ###</span><br><span class="line">tic &#x3D; time.process_time()</span><br><span class="line">outer &#x3D; np.outer(x1,x2)</span><br><span class="line">toc &#x3D; time.process_time()</span><br><span class="line">print (&quot;outer &#x3D; &quot; + str(outer) + &quot;\n ----- Computation time &#x3D; &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br><span class="line">### VECTORIZED ELEMENTWISE MULTIPLICATION ###</span><br><span class="line">tic &#x3D; time.process_time()</span><br><span class="line">mul &#x3D; np.multiply(x1,x2)</span><br><span class="line">toc &#x3D; time.process_time()</span><br><span class="line">print (&quot;elementwise multiplication &#x3D; &quot; + str(mul) + &quot;\n ----- Computation time &#x3D; &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br><span class="line">### VECTORIZED GENERAL DOT PRODUCT ###</span><br><span class="line">tic &#x3D; time.process_time()</span><br><span class="line">dot &#x3D; np.dot(W,x1)</span><br><span class="line">toc &#x3D; time.process_time()</span><br><span class="line">print (&quot;gdot &#x3D; &quot; + str(dot) + &quot;\n ----- Computation time &#x3D; &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br></pre></td></tr></table></figure>

<p>用L1范数作为损失函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def L1(yhat, y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments:</span><br><span class="line">    yhat -- vector of size m (predicted labels)</span><br><span class="line">    y -- vector of size m (true labels)</span><br><span class="line">    Returns:</span><br><span class="line">    loss -- the value of the L1 loss function defined above</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    loss &#x3D; np.sum(abs(y-yhat))</span><br><span class="line">    return loss</span><br></pre></td></tr></table></figure>

<p>测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yhat &#x3D; np.array([.9, 0.2, 0.1, .4, .9])</span><br><span class="line">y &#x3D; np.array([1, 0, 0, 1, 1])</span><br><span class="line">print(&quot;L1 &#x3D; &quot; + str(L1(yhat,y)))</span><br></pre></td></tr></table></figure>

<p>用L2范数作为损失函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def L2(yhat, y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments:</span><br><span class="line">    yhat -- vector of size m (predicted labels)</span><br><span class="line">    y -- vector of size m (true labels)</span><br><span class="line">    Returns:</span><br><span class="line">    loss -- the value of the L2 loss function defined above</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    loss &#x3D; np.sum(np.dot(y-yhat,y-yhat))</span><br><span class="line">    return loss</span><br></pre></td></tr></table></figure>

<p>测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yhat &#x3D; np.array([.9, 0.2, 0.1, .4, .9])</span><br><span class="line">y &#x3D; np.array([1, 0, 0, 1, 1])</span><br><span class="line">print(&quot;L2 &#x3D; &quot; + str(L2(yhat,y)))</span><br></pre></td></tr></table></figure>
      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Andrew/" rel="tag">Andrew</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-tensorflow2-0系列-四-：多层感知机" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/17/tensorflow2-0%E7%B3%BB%E5%88%97-%E5%9B%9B-%EF%BC%9A%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"
    >tensorflow2.0系列(四)：多层感知机</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/17/tensorflow2-0%E7%B3%BB%E5%88%97-%E5%9B%9B-%EF%BC%9A%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/" class="article-date">
  <time datetime="2020-03-17T10:07:51.000Z" itemprop="datePublished">2020-03-17</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>多层感知机就是多层全连接神经网络。<br>在这里使用多层感知机完成 MNIST 手写体数字图片数据集 [LeCun1998] 的分类任务。<br>我们将依次完成以下几个环节。<br>1、使用 tf.keras.datasets 获得数据集并预处理<br>2、使用 tf.keras.Model 和 tf.keras.layers 构建模型<br>3、构建模型训练流程，使用 tf.keras.losses 计算损失函数，并使用 tf.keras.optimizer 优化模型<br>4、构建模型评估流程，使用 tf.keras.metrics 计算评估指标<br>第一步，先定义一个MNISTLoader类来下载MNIST数据集，预处理以及自定义一个从数据集中取出数据的方法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class MNISTLoader():</span><br><span class="line">    def __init__(self):</span><br><span class="line">        mnist &#x3D; tf.keras.datasets.mnist</span><br><span class="line">        (self.train_data, self.train_label), (self.test_data, self.test_label) &#x3D; mnist.load_data()</span><br><span class="line">        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道</span><br><span class="line">        self.train_data &#x3D; np.expand_dims(self.train_data.astype(np.float32) &#x2F; 255.0, axis&#x3D;-1)      # [60000, 28, 28, 1]</span><br><span class="line">        self.test_data &#x3D; np.expand_dims(self.test_data.astype(np.float32) &#x2F; 255.0, axis&#x3D;-1)        # [10000, 28, 28, 1]</span><br><span class="line">        self.train_label &#x3D; self.train_label.astype(np.int32)    # [60000]</span><br><span class="line">        self.test_label &#x3D; self.test_label.astype(np.int32)      # [10000]</span><br><span class="line">        self.num_train_data, self.num_test_data &#x3D; self.train_data.shape[0], self.test_data.shape[0]</span><br><span class="line"></span><br><span class="line">    def get_batch(self, batch_size):</span><br><span class="line">        # 从数据集中随机取出batch_size个元素并返回</span><br><span class="line">        index &#x3D; np.random.randint(0, np.shape(self.train_data)[0], batch_size)</span><br><span class="line">        return self.train_data[index, :], self.train_label[index]</span><br></pre></td></tr></table></figure>
<p>需要注意的是keras内置的load_data()方法返回的是两个元组以及get_batch()方法是怎么写的。</p>
<p>在 TensorFlow 中，图像数据集一般表示成一个[图像数目，长，宽，色彩通道数] 的四维张量。这里读入的是灰度图片，色彩通道数为1(彩色RGB图像色彩通道数为3)。由于load_data()方法返回的都是[图像数目，长，宽]的三维张量，所以使用np.expand_dims()函数给返回张量在最后添加一维，保持一致性。<br>多层感知机的模型与之前实现的线性模型不同的地方在于层数增加了，以及引入了非线性激活函数ReLU函数。该模型输入一个向量，输出一个10维的向量，每一维分别是识别为0-9的概率。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class MLP(tf.keras.Model):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.flatten &#x3D; tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平</span><br><span class="line">        self.dense1 &#x3D; tf.keras.layers.Dense(units&#x3D;100, activation&#x3D;tf.nn.relu)</span><br><span class="line">        self.dense2 &#x3D; tf.keras.layers.Dense(units&#x3D;10)</span><br><span class="line"></span><br><span class="line">    def call(self, inputs):         # [batch_size, 28, 28, 1]</span><br><span class="line">        x &#x3D; self.flatten(inputs)    # [batch_size, 784]</span><br><span class="line">        x &#x3D; self.dense1(x)          # [batch_size, 100]</span><br><span class="line">        x &#x3D; self.dense2(x)          # [batch_size, 10]</span><br><span class="line">        output &#x3D; tf.nn.softmax(x)</span><br><span class="line">        return output</span><br></pre></td></tr></table></figure>
<p>因为我们希望输出的向量具有这样两个特点：该向量中的每个元素均在[0,1]之间；该向量的所有元素之和为1。因此我们对最后一层的线性输出使用Softmax函数进行归一化。softmax不仅取出了向量中的最大值，还保留了对其余类别的预测信息，因此被称作soft argmax。<br>归一化就是把数值映射到(0,1)之间，有很多种方法。</p>
<p>第三步，就是构建模型训练流程，首先需要定义一些超参数，比如训练的轮数、一次喂入多少个数据，即batch_size、还有学习率。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num_epochs &#x3D; 5</span><br><span class="line">batch_size &#x3D; 50</span><br><span class="line">learning_rate &#x3D; 0.001</span><br></pre></td></tr></table></figure>
<p>接着将前两步定义的两个类实例化，并且实例化出一个优化器，这里采用的Adam优化器。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model &#x3D; MLP()</span><br><span class="line">data_loader &#x3D; MNISTLoader()</span><br><span class="line">optimizer &#x3D; tf.keras.optimizers.Adam(learning_rate&#x3D;learning_rate)</span><br></pre></td></tr></table></figure>
<p>接下来就是迭代进行一下步骤：<br>从 DataLoader 中随机取一批训练数据；<br>将这批数据送入模型，计算出模型的预测值；<br>将模型预测值与真实值进行比较，计算损失函数(loss)。这里使用tf.keras.losses中的交叉熵函数作为损失函数；<br>计算损失函数关于模型变量的导数；<br>将求出的导数值传入优化器，使用优化器的 apply_gradients 方法更新模型参数以最小化损失函数。<br>具体代码为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">num_batches &#x3D; int(data_loader.num_train_data &#x2F;&#x2F; batch_size * num_epochs)</span><br><span class="line">for batch_index in range(num_batches):</span><br><span class="line">    X, y &#x3D; data_loader.get_batch(batch_size)</span><br><span class="line">    with tf.GradientTape() as tape:</span><br><span class="line">        y_pred &#x3D; model(X)</span><br><span class="line">        loss &#x3D; tf.keras.losses.sparse_categorical_crossentropy(y_true&#x3D;y, y_pred&#x3D;y_pred)</span><br><span class="line">        loss &#x3D; tf.reduce_mean(loss)</span><br><span class="line">        print(&quot;batch %d: loss %f&quot; % (batch_index, loss.numpy()))</span><br><span class="line">    grads &#x3D; tape.gradient(loss, model.variables)</span><br><span class="line">    optimizer.apply_gradients(grads_and_vars&#x3D;zip(grads, model.variables))</span><br></pre></td></tr></table></figure>
<p>在这里，我们没有显式地写出一个损失函数，而是使用了tf.keras.losses中的sparse_categorical_crossentropy(交叉熵)函数，将模型的预测值y_pred与真实的标签值y作为函数参数传入，由Keras帮助我们计算损失函数的值。<br>交叉熵作为损失函数，在分类问题中被广泛应用。<br>第四步，评估模型。keras在metrics模块中封装了不同的评估器用于适应不同的需求。在这里我们使用的是SparseCategoricalAccuracy评估器来评估模型在测试集上的性能，该评估器输出的是预测正确的样本数占总样本数的比例。<br>在每次迭代测试集的时候，都会通过update_state()方法向评估器输入两个参数：y_pred和y_true，即模型预测出的结果和真实结果。评估器定义了内部变量来保存当前评估指标相关的参数数值(例如当前已传入的累计样本数和当前预测正确的样本数)。迭代结束后，我们使用result()方法输出最终的评估指标值(预测正确的样本数占总样本数的比例)。<br>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sparse_categorical_accuracy &#x3D; tf.keras.metrics.SparseCategoricalAccuracy()</span><br><span class="line">num_batches &#x3D; int(data_loader.num_test_data &#x2F;&#x2F; batch_size)</span><br><span class="line">for batch_index in range(num_batches):</span><br><span class="line">    start_index, end_index &#x3D; batch_index * batch_size, (batch_index + 1) * batch_size</span><br><span class="line">    y_pred &#x3D; model.predict(data_loader.test_data[start_index: end_index])</span><br><span class="line">    sparse_categorical_accuracy.update_state(y_true&#x3D;data_loader.test_label[start_index: end_index], y_pred&#x3D;y_pred)</span><br><span class="line">print(&quot;test accuracy: %f&quot; % sparse_categorical_accuracy.result())</span><br></pre></td></tr></table></figure>
<p>在这里我们首先实例化了一个 tf.keras.metrics.SparseCategoricalAccuracy 评估器，并且使用For循环迭代分批次传入了测试集数据的预测结果与真实结果，并调用result()方法输出了训练后的模型在测试数据集上的准确率。<br>但是同样可以看到keras的缺点就是啥都帮你做了你就很难知道背后的机理是怎么实现的了。</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-tensorflow2-0系列-三" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/17/tensorflow2-0%E7%B3%BB%E5%88%97-%E4%B8%89/"
    >tensorflow2.0系列(三)</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/17/tensorflow2-0%E7%B3%BB%E5%88%97-%E4%B8%89/" class="article-date">
  <time datetime="2020-03-17T04:58:42.000Z" itemprop="datePublished">2020-03-17</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>在keras中，通过继承tf.keras.Model来定义自己的模型。在继承类中，需要重写”<del><strong>init</strong></del>“()和和call(input)两个方法，也可以根据需要增加自定义的方法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class MyModel(tf.keras.Model):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()     # Python 2 下使用 super(MyModel, self).__init__()</span><br><span class="line">        &#x2F;&#x2F; 此处添加初始化代码（包含 call 方法中会用到的层），例如</span><br><span class="line">        &#x2F;&#x2F; layer1 &#x3D; tf.keras.layers.BuiltInLayer(...)</span><br><span class="line">        &#x2F;&#x2F; layer2 &#x3D; MyCustomLayer(...)</span><br><span class="line"></span><br><span class="line">    def call(self, input):</span><br><span class="line">        &#x2F;&#x2F; 此处添加模型调用的代码（处理输入并返回输出），例如</span><br><span class="line">        &#x2F;&#x2F; x &#x3D; layer1(input)</span><br><span class="line">        &#x2F;&#x2F; output &#x3D; layer2(x)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 还可以添加自定义的方法</span><br></pre></td></tr></table></figure>
<p>继承 tf.keras.Model 后，就以使用父类的方法和属性，比如在实例化类model = Model()后，就可以通过model.variables这一属性直接获得模型中的所有变量，而不需要我们一个个显式地指定变量。<br>用keras实现线性回归：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">X &#x3D; tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])</span><br><span class="line">y &#x3D; tf.constant([[10.0], [20.0]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Linear(tf.keras.Model):</span><br><span class="line">    def &quot;&#39;__init__&#39;&quot;(self):</span><br><span class="line">        super().&quot;&#39;__init__&#39;&quot;()</span><br><span class="line">        self.dense &#x3D; tf.keras.layers.Dense(</span><br><span class="line">            units&#x3D;1,</span><br><span class="line">            activation&#x3D;None,</span><br><span class="line">            kernel_initializer&#x3D;tf.zeros_initializer(),</span><br><span class="line">            bias_initializer&#x3D;tf.zeros_initializer()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def call(self, input):</span><br><span class="line">        output &#x3D; self.dense(input)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 以下代码结构与前节类似</span><br><span class="line">model &#x3D; Linear()</span><br><span class="line">optimizer &#x3D; tf.keras.optimizers.SGD(learning_rate&#x3D;0.01)</span><br><span class="line">for i in range(100):</span><br><span class="line">    with tf.GradientTape() as tape:</span><br><span class="line">        y_pred &#x3D; model(X)      # 调用模型 y_pred &#x3D; model(X) 而不是显式写出 y_pred &#x3D; a * X + b</span><br><span class="line">        loss &#x3D; tf.reduce_mean(tf.square(y_pred - y))</span><br><span class="line">    grads &#x3D; tape.gradient(loss, model.variables)    # 使用 model.variables 这一属性直接获得模型中的所有变量</span><br><span class="line">    optimizer.apply_gradients(grads_and_vars&#x3D;zip(grads, model.variables))</span><br><span class="line">print(model.variables)</span><br></pre></td></tr></table></figure>
<p>tf.keras.layers.Dense类的作用是实例化生成全连接层，它的参数有：<br>units ：输出张量的维度<br>activation ：激活函数，对应于f(AW + b)中的f，默认为无激活函数(a(x) = x)。常用的激活函数包括 tf.nn.relu、tf.nn.tanh和tf.nn.sigmoid；<br>use_bias ：是否加入偏置向量bias，即f(AW + b)中的b。默认为True；<br>kernel_initializer、bias_initializer：权重矩阵kernel和偏置向量bias两个变量的初始化器。默认为tf.glorot_uniform_initializer。设置为tf.zeros_initializer表示将两个变量均初始化为全0；<br>该层包含权重矩阵kernel=[input_dim,units]和偏置向量bias=[units]两个可训练变量，对应于f(AW+b)中的W和b。<br>tf.matmul(input, kernel) 的结果是一个形状为 [batch_size, units] 的二维矩阵，而bias是一个形状为[units]的一维向量，它们是怎么相加的呢？<br>这里用到的是python的广播机制。<br>为什么模型类是重写call()方法而不是”‘<strong>call</strong>‘“()方法？<br>实例化也可以理解为调用这个类。<br>在python中，myClass()==myClass.”‘<strong>call</strong>‘“()。重写call()方法而不是”‘<strong>call</strong>‘“()方法是因为在keras中call()方法是在”‘<strong>call</strong>‘“()方法中被调用的，也就是说，在keras框架下，在调用某个模型时，keras要执行的不仅仅是用户定义的call()方法，还有一些每个模型在调用时都要进行的处理，而这些东西就写在了””<strong>call</strong>‘“()方法下。因此用户重写的只能是call()方法。</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-白板推导系列-一-：SVM定义" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/16/%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97-%E4%B8%80-%EF%BC%9ASVM%E5%AE%9A%E4%B9%89/"
    >白板推导系列(一)：SVM定义</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/16/%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97-%E4%B8%80-%EF%BC%9ASVM%E5%AE%9A%E4%B9%89/" class="article-date">
  <time datetime="2020-03-16T10:12:51.000Z" itemprop="datePublished">2020-03-16</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p><img src="/images/SVMdefinition.jpg" alt=""></p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      

    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-tensorflow2-0系列-二" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/16/tensorflow2-0%E7%B3%BB%E5%88%97-%E4%BA%8C/"
    >tensorflow2.0系列(二)</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/16/tensorflow2-0%E7%B3%BB%E5%88%97-%E4%BA%8C/" class="article-date">
  <time datetime="2020-03-16T05:55:04.000Z" itemprop="datePublished">2020-03-16</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>random_float = tf.random.uniform(shape=()) # 定义一个随机数（标量）</p>
<p>zero_vector = tf.zeros(shape=(2)) # 定义一个有2个元素的零向量</p>
<p>A = tf.constant([[1., 2.], [3., 4.]])<br>B = tf.constant([[5., 6.], [7., 8.]]) # 定义两个2×2的常量矩阵<br>注意shape的值，shape括号里数字的个数就是生成的张量的维度，没有数字就是0维，也就是标量。<br>张量一般关注三个地方：形状、类型和值。通过张量的 shape 、 dtype 属性和 numpy() 方法获得。</p>
<p>print(A.shape)      # 输出(2, 2)，即矩阵的长和宽均为2<br>print(A.dtype)      # 输出&lt;dtype: ‘float32’&gt;<br>print(A.numpy())    # 输出[[1. 2.],[3. 4.]]  # 查看矩阵A的形状、类型和值</p>
<p>张量通过numpy方法取出张量的值。<br>在tensorflow1.x版本中，要开启即时执行模式要在开启的位置调用tf.enable_eager_execution() 函数，在tensorflow2.x版本<br>中，即时执行模式是默认开启的。另外，关闭即时执行模式就是调用tf.compat.v1.disable_eager_execution() 函数。不清楚<br>是不是在1.x、2.x版本中都是这么来关闭这种模式。2.x版本里是可行的。<br>即时执行模式和图执行模式的区别就是进行运算不用调用会话了，直接就可以计算。<br>tensorflow的自动求导功能：<br>例：使用 tf.GradientTape() 计算函数 y(x) = x^2 在 x = 3 时的导数<br>import tensorflow as tf</p>
<p>x = tf.Variable(initial_value=3.)<br>with tf.GradientTape() as tape:     # 在 tf.GradientTape() 的上下文内，所有计算步骤都会被记录以用于求导<br>    y = tf.square(x)<br>y_grad = tape.gradient(y, x)        # 计算y关于x的导数<br>print([y, y_grad])<br>例：使用 tf.GradientTape() 计算函数 L(w, b) = (Xw + b - y)^2 在 w = (1, 2)^T, b = 1 时分别对 w, b 的偏导数。<br>其中 X = [[1.,2.],[3.,4.]],y = [[1.],[2.]]<br>X = tf.constant([[1., 2.], [3., 4.]])<br>y = tf.constant([[1.], [2.]])<br>w = tf.Variable(initial_value=[[1.], [2.]])<br>b = tf.Variable(initial_value=1.)<br>with tf.GradientTape() as tape:<br>    L = 0.5 * tf.reduce_sum(tf.square(tf.matmul(X, w) + b - y))<br>w_grad, b_grad = tape.gradient(L, [w, b])        # 计算L(w, b)关于w, b的偏导数<br>print([L.numpy(), w_grad.numpy(), b_grad.numpy()])<br>#tf.reduce_sum() 操作是对输入张量的所有元素求和，输出一个形状为空的纯量张量（可以通过 axis 参数来指定求和的维度，不指定则默认对所有元素求和）。<br>波士顿房价问题，什么时候要进行归一化操作，归一化的方式是唯一的吗？<br>import numpy as np</p>
<p>X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype=np.float32)<br>y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)</p>
<p>X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())<br>y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())<br>使用tensorflow实现城市房价问题的参数求解：<br>X = tf.constant(X)<br>y = tf.constant(y)</p>
<p>a = tf.Variable(initial_value=0.)<br>b = tf.Variable(initial_value=0.)<br>variables = [a, b]</p>
<p>num_epoch = 10000<br>optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)<br>for e in range(num_epoch):<br>    # 使用tf.GradientTape()记录损失函数的梯度信息<br>    with tf.GradientTape() as tape:<br>        y_pred = a * X + b<br>        loss = 0.5 * tf.reduce_sum(tf.square(y_pred - y))<br>    # TensorFlow自动计算损失函数关于自变量（模型参数）的梯度<br>    grads = tape.gradient(loss, variables)<br>    # TensorFlow自动根据梯度更新参数<br>    optimizer.apply_gradients(grads_and_vars=zip(grads, variables))</p>
<p>print(a, b)<br>Python 的 zip() 函数：<br>如果 a = [1, 3, 5]， b = [2, 4, 6]，那么 zip(a, b) = [(1, 2), (3, 4), …, (5, 6)]。</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-tensorflow2-0系列-一" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/16/tensorflow2-0%E7%B3%BB%E5%88%97-%E4%B8%80/"
    >tensorflow2.0系列(一)</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/16/tensorflow2-0%E7%B3%BB%E5%88%97-%E4%B8%80/" class="article-date">
  <time datetime="2020-03-16T04:40:17.000Z" itemprop="datePublished">2020-03-16</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>tensorflow安装：<br>      1、Anaconda<br>      2、conda create –name tf2 python=3.7      # “tf2”是你建立的conda虚拟环境的名字<br>         conda activate tf2                      # 进入名为“tf2”的conda虚拟环境<br>      3、pip install tensorflow -i <a href="https://mirrors.tuna.tsinghua.edu.cn/help/pypi/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/help/pypi/</a><br>注：<br>      1、也可以使用 conda install tensorflow 来安装 TensorFlow，但是conda源的版本更新较慢<br>      2、从tensorflow2.1开始，tensorflow包就是gpu版本，不需要安装tensorflow-gpu包，<br>         如果需要安装cpu版本，则安装tensorflow-cpu<br>pip常用命令：<br>      pip install [package-name]              # 安装名为[package-name]的包<br>      pip install [package-name]==X.X         # 安装名为[package-name]的包并指定版本X.X<br>      pip install [package-name] –proxy=代理服务器IP:端口号         # 使用代理服务器安装<br>      pip install [package-name] –upgrade    # 更新名为[package-name]的包<br>      pip uninstall [package-name]            # 删除名为[package-name]的包<br>      pip list                                # 列出当前环境下已安装的所有包<br>conda常用命令：<br>      conda install [package-name]        # 安装名为[package-name]的包<br>      conda install [package-name]=X.X    # 安装名为[package-name]的包并指定版本X.X<br>      conda update [package-name]         # 更新名为[package-name]的包<br>      conda remove [package-name]         # 删除名为[package-name]的包<br>      conda list                          # 列出当前环境下已安装的所有包<br>      conda search [package-name]         # 列出名为[package-name]的包在conda源中的所有可用版本<br>conda包管理器相比pip的优点是可以安装更多的包，但是缺点是版本更新较慢<br>创建虚拟环境命令：<br>      conda create –name [env-name]      # 建立名为[env-name]的Conda虚拟环境<br>      conda activate [env-name]           # 进入名为[env-name]的Conda虚拟环境<br>      conda deactivate                    # 退出当前的Conda虚拟环境<br>      conda env remove –name [env-name]  # 删除名为[env-name]的Conda虚拟环境<br>      conda env list                      # 列出所有Conda虚拟环境<br>tensorflowgpu版本安装：<br>      conda install cudatoolkit=X.X<br>      conda install cudnn=X.X.X<br>      cuda与cudnn，tensorflow-gpu包的版本一定要对应的上<br>      也可以手动去官网下载cuda和cudnn<br>      另外，在用conda包管理器安装tensorflow-gpu版本时会自动安装对应版本的cuda与cudnn，<br>      缺点是tensorflow-gpu的版本可能较低，因为conda源里的包更新的比较慢。<br>reference:<a href="https://tf.wiki/" target="_blank" rel="noopener">https://tf.wiki/</a></p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-系列-二-：线性分类器" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/15/%E7%B3%BB%E5%88%97-%E4%BA%8C-%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/"
    >系列(二)：线性分类器</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/15/%E7%B3%BB%E5%88%97-%E4%BA%8C-%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/" class="article-date">
  <time datetime="2020-03-15T03:53:43.000Z" itemprop="datePublished">2020-03-15</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>神经网络就像乐高玩具，搭积木，非线性神经元就是一块块积木。再比如设计一个输入图片输出描述性语言的系统，我们就可以用一个CNN和一个RNN作为基本组件来搭建。<br>Y=WX+b中的b是怎么起作用的，有时候你的数据集是不平衡的，比如猫的个数多于狗的个数。个数更多的类别往往有更大的偏置项。[1]<br>线性分类器的工作方式就是X是一个图像，把X展平成一个列向量，W是一个以列向量的长度与分类的个数为维度的二维向量，b是一个长度为类别个数的列向量。经过运算后会得到列向量Y，Y的元素就是每个类别的得分。<br>哪个类别的得分越高就被划分为哪类。W的每一行就是每一种类别的模板，模板与图像的点乘结果大，就说这个模板匹配了图像，也就意味着这个图像中包含模板想要寻找的特征。<br>把这些模板可视化，找到模板中的特征，就是这个分类器在分类时想要寻找的特征。<br>线性分类器的缺点，模板是在数据集上训练得到的，所以如果某个类别的图像是对称过来的双份数据，那么这个类别训练出的模板可视化之后的结果可能会很奇怪，比如有两个头的马。再比如开向不同方向的火车，模板想要<br>在一张图像上把所有的方向特征都表现出来，就会看起来很奇怪，甚至看起来像一张由噪声生成的图片。线性分类器只能绘制出一条线性边界，这就是它的局限性。</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs231n/" rel="tag">cs231n</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
  </article>
  

  
  <nav class="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/3/">上一页</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/5/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2015-2020
        John Doe
      </li>
      <li>
        
        Powered by
        
        
        <a href="https://hexo.io" target="_blank">Hexo</a> Theme <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <span>
  <i>PV:<span id="busuanzi_value_page_pv"></span></i>
  <i>UV:<span id="busuanzi_value_site_uv"></span></i>
</span>
        
      </li>
      
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
    <aside class="sidebar">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="畅院士的开山大弟子的博客"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E6%97%85%E8%A1%8C/">旅行</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="http://shenyu-vip.lofter.com" target="_blank" rel="noopener">摄影</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2019/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/share.js"></script>


<script src="/js/lazyload.min.js"></script>


<script>
  try {
    var typed = new Typed("#subtitle", {
      strings: ['面朝大海，春暖花开', '愿你一生努力，一生被爱', '想要的都拥有，得不到的都释怀'],
      startDelay: 0,
      typeSpeed: 200,
      loop: true,
      backSpeed: 100,
      showCursor: true
    });
  } catch (err) {
  }

</script>





<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/js/ayer.js"></script>



<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>




<script src="/js/busuanzi-2.3.pure.min.js"></script>



<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.2/dist/jquery.fancybox.min.css">
<script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.2/dist/jquery.fancybox.min.js"></script>


    
  </div>
</body>

</html>