<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
     畅院士的开山大弟子的博客
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/css/main.css">

  
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
  
  

  

</head>

</html>

<body>
  <div id="app">
    <main class="content">
      
<section class="cover">
    
      
      <a class="forkMe" href="https://github.com/Shen-Yu/hexo-theme-ayer"
        target="_blank"><img width="149" height="149" src="/images/forkme.png"
          class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/cover1.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">畅院士的开山大弟子的博客</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>

<div id="main">
  <section class="outer">
  <article class="articles">
    
    
    
    
    <article id="post-tensorflow2-0系列-九-：数据集的构建与预处理" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/20/tensorflow2-0%E7%B3%BB%E5%88%97-%E4%B9%9D-%EF%BC%9A%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E6%9E%84%E5%BB%BA%E4%B8%8E%E9%A2%84%E5%A4%84%E7%90%86/"
    >tensorflow2.0系列(九)：数据集的构建与预处理</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/20/tensorflow2-0%E7%B3%BB%E5%88%97-%E4%B9%9D-%EF%BC%9A%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E6%9E%84%E5%BB%BA%E4%B8%8E%E9%A2%84%E5%A4%84%E7%90%86/" class="article-date">
  <time datetime="2020-03-20T06:33:46.000Z" itemprop="datePublished">2020-03-20</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>很多时候，我们希望使用自己的数据集来训练模型。然而，面对一堆格式不一的原始数据文件，将其预处理并读入程序的过程往往十分繁琐，甚至比模型的设计还要耗费精力。比如，为了读入一批图像文件，我们可能需要纠结于python的各种图像处理包（比如pillow），自己设计 Batch 的生成方式，最后还可能在运行的效率上不尽如人意。为此，TensorFlow提供了tf.data这一模块，包括了一套灵活的数据集构建 API，能够帮助我们快速、高效地构建数据输入的流水线，尤其适用于数据量巨大的场景。<br>数据集对象的建立：<br>tf.data的核心是tf.data.Dataset类，提供了对数据集的高层封装。tf.data.Dataset由一系列的可迭代访问的元素（element）组成，每个元素包含一个或多个张量。比如说，对于一个由图像组成的数据集，每个元素可以是一个形状为长×宽×通道数的图片张量，也可以是由图片张量和图片标签张量组成的元组（Tuple）。<br>最基础的建立tf.data.Dataset的方法是使用tf.data.Dataset.from_tensor_slices()，适用于数据量较小（能够整个装进内存）的情况。具体而言，如果我们的数据集中的所有元素通过张量的第0维，拼接成一个大的张量（例如，前节的MNIST数据集的训练集即为一个[60000,28,28,1]的张量，表示了60000张28*28的单通道灰度图像），那么我们提供一个这样的张量或者第0维大小相同的多个张量作为输入，即可按张量的第0维展开来构建数据集，数据集的元素数量为张量第0位的大小。具体示例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">X &#x3D; tf.constant([2013, 2014, 2015, 2016, 2017])</span><br><span class="line">Y &#x3D; tf.constant([12000, 14000, 15000, 16500, 17500])</span><br><span class="line"></span><br><span class="line"># 也可以使用NumPy数组，效果相同</span><br><span class="line"># X &#x3D; np.array([2013, 2014, 2015, 2016, 2017])</span><br><span class="line"># Y &#x3D; np.array([12000, 14000, 15000, 16500, 17500])</span><br><span class="line"></span><br><span class="line">dataset &#x3D; tf.data.Dataset.from_tensor_slices((X, Y))</span><br><span class="line"></span><br><span class="line">for x, y in dataset:</span><br><span class="line">    print(x.numpy(), y.numpy())</span><br></pre></td></tr></table></figure>
<p>当提供多个张量作为输入时，张量的第0维大小必须相同，且必须将多个张量作为元组（Tuple，即使用Python中的小括号）拼接并作为输入。<br>以上一节的MNIST数据集为例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt </span><br><span class="line"></span><br><span class="line">(train_data, train_label), (_, _) &#x3D; tf.keras.datasets.mnist.load_data()</span><br><span class="line">train_data &#x3D; np.expand_dims(train_data.astype(np.float32) &#x2F; 255.0, axis&#x3D;-1)      # [60000, 28, 28, 1]</span><br><span class="line">mnist_dataset &#x3D; tf.data.Dataset.from_tensor_slices((train_data, train_label))</span><br><span class="line"></span><br><span class="line">for image, label in mnist_dataset:</span><br><span class="line">    plt.title(label.numpy())</span><br><span class="line">    plt.imshow(image.numpy()[:, :, 0])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<p>高维数组的切片操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X[:,0]是numpy中数组的一种写法，表示对一个二维数组，取该二维数组第一维中的所有数据，第二维中取第0个数据，直观来说，X[:,0]就是取所有行的第0个数据, X[:,1]就是取所有行的第1个数据。</span><br></pre></td></tr></table></figure>

<p>TensorFlow Datasets提供了一个基于tf.data.Datasets的开箱即用的数据集集合，相关内容可参考<a href="https://tf.wiki/zh/appendix/tfds.html" target="_blank" rel="noopener">TensorFlow Datasets</a>。例如，使用以下语句：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow_datasets as tfds</span><br><span class="line">dataset &#x3D; tfds.load(&quot;mnist&quot;, split&#x3D;tfds.Split.TRAIN)</span><br></pre></td></tr></table></figure>
<p>即可快速载入 MNIST 数据集。</p>
<p>对于特别巨大而无法完整载入内存的数据集，我们可以先将数据集处理为TFRecord格式，然后使用tf.data.TFRocrdDataset()进行载入。详情请参考下一节。<br>数据集对象的预处理：<br>tf.data.Dataset类为我们提供了多种数据集预处理方法。最常用的有：<br>Dataset.map(f)：对数据集中的每个元素应用函数f，得到一个新的数据集（这部分往往结合tf.io进行读写和解码文件，tf.image进行图像处理）；<br>Dataset.shuffle(buffer_size)：将数据集打乱（设定一个固定大小的缓冲区（Buffer），取出前 buffer_size个元素放入，并从缓冲区中随机采样，采样后的数据用后续数据替换）；<br>Dataset.batch(batch_size)：将数据集分成批次，即对每batch_size个元素，使用tf.stack()在第0维合并，成为一个元素；<br>除此以外，还有Dataset.repeat()（重复数据集的元素）、Dataset.reduce()（与 Map 相对的聚合操作）、Dataset.take()（截取数据集中的前若干个元素）等，可参考<a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset" target="_blank" rel="noopener">API文档</a> 进一步了解。</p>
<p>以下以 MNIST 数据集进行示例。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def rot90(image, label):</span><br><span class="line">    image &#x3D; tf.image.rot90(image)</span><br><span class="line">    return image, label</span><br><span class="line"></span><br><span class="line">mnist_dataset &#x3D; mnist_dataset.map(rot90)</span><br><span class="line"></span><br><span class="line">for image, label in mnist_dataset:</span><br><span class="line">    plt.title(label.numpy())</span><br><span class="line">    plt.imshow(image.numpy()[:, :, 0])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<p>使用Dataset.batch()将数据集划分批次，每个批次的大小为4：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mnist_dataset &#x3D; mnist_dataset.batch(4)</span><br><span class="line"></span><br><span class="line">for images, labels in mnist_dataset:    # image: [4, 28, 28, 1], labels: [4]</span><br><span class="line">    fig, axs &#x3D; plt.subplots(1, 4)</span><br><span class="line">    for i in range(4):</span><br><span class="line">        axs[i].set_title(labels.numpy()[i])</span><br><span class="line">        axs[i].imshow(images.numpy()[i, :, :, 0])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<p>使用Dataset.shuffle()将数据打散后再设置批次，缓存大小设置为 10000：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mnist_dataset &#x3D; mnist_dataset.shuffle(buffer_size&#x3D;10000).batch(4)</span><br><span class="line"></span><br><span class="line">for images, labels in mnist_dataset:</span><br><span class="line">    fig, axs &#x3D; plt.subplots(1, 4)</span><br><span class="line">    for i in range(4):</span><br><span class="line">        axs[i].set_title(labels.numpy()[i])</span><br><span class="line">        axs[i].imshow(images.numpy()[i, :, :, 0])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>经测试，第一次运行和第二次运行的结果不同，可见每次的数据都会被随机打散。</p>
<p>Dataset.shuffle()时缓冲区大小buffer_size的设置：<br>tf.data.Dataset作为一个针对大规模数据设计的迭代器，本身无法方便地获得自身元素的数量或随机访问元素。因此，为了高效且较为充分地打散数据集，需要一些特定的方法。Dataset.shuffle()采取了以下方法：<br>设定一个固定大小为buffer_size的缓冲区（Buffer）；<br>初始化时，取出数据集中的前buffer_size个元素放入缓冲区；<br>每次需要从数据集中取元素时，即从缓冲区中随机采样一个元素并取出，然后从后续的元素中取出一个放回到之前被取出的位置，以维持缓冲区的大小。<br>因此，缓冲区的大小需要根据数据集的特性和数据排列顺序特点来进行合理的设置。比如：<br>当buffer_size设置为1时，其实等价于没有进行任何打散；<br>当数据集的标签顺序分布极为不均匀（例如二元分类时数据集前N个的标签为0，后N个的标签为1）时，较小的缓冲区大小会使得训练时取出的Batch数据很可能全为同一标签，从而影响训练效果。一般而言，数据集的顺序分布若较为随机，则缓冲区的大小可较小，否则则需要设置较大的缓冲区。</p>
<p>使用 tf.data 的并行化策略提高训练流程效率：<br>当训练模型时，我们希望充分利用计算资源，减少CPU/GPU的空载时间。然而有时，数据集的准备处理非常耗时，使得我们在每进行一次训练前都需要花费大量的时间准备待训练的数据，而此时GPU只能空载而等待数据，造成了计算资源的浪费，如下图所示：<br>[1]</p>
<p>数据集元素的获取与使用:<br>构建好数据并预处理后，我们需要从其中迭代获取数据以用于训练。tf.data.Dataset是一个Python的可迭代对象，因此可以使用For循环迭代获取数据，即：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataset &#x3D; tf.data.Dataset.from_tensor_slices((A, B, C, ...))</span><br><span class="line">for a, b, c, ... in dataset:</span><br><span class="line">    # 对张量a, b, c等进行操作，例如送入模型进行训练</span><br></pre></td></tr></table></figure>

<p>也可以使用iter()显式创建一个Python迭代器并使用next()获取下一个元素，即：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dataset &#x3D; tf.data.Dataset.from_tensor_slices((A, B, C, ...))</span><br><span class="line">it &#x3D; iter(dataset)</span><br><span class="line">a_0, b_0, c_0, ... &#x3D; next(it)</span><br><span class="line">a_1, b_1, c_1, ... &#x3D; next(it)</span><br></pre></td></tr></table></figure>

<p>[2]</p>
<p>实例：cats_vs_dogs图像分类:<br>以下代码以猫狗图片二分类任务为示例，展示了使用tf.data结合tf.io和tf.image建立tf.data.Dataset数据集，并进行训练和测试的完整过程。数据集可至<a href="https://www.floydhub.com/fastai/datasets/cats-vs-dogs" target="_blank" rel="noopener">这里</a>下载。使用前须将数据集解压到代码中data_dir所设置的目录（此处默认设置为C:/datasets/cats_vs_dogs，可根据自己的需求进行修改）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">num_epochs &#x3D; 10</span><br><span class="line">batch_size &#x3D; 32</span><br><span class="line">learning_rate &#x3D; 0.001</span><br><span class="line">data_dir &#x3D; &#39;C:&#x2F;datasets&#x2F;cats_vs_dogs&#39;</span><br><span class="line">train_cats_dir &#x3D; data_dir + &#39;&#x2F;train&#x2F;cats&#x2F;&#39;</span><br><span class="line">train_dogs_dir &#x3D; data_dir + &#39;&#x2F;train&#x2F;dogs&#x2F;&#39;</span><br><span class="line">test_cats_dir &#x3D; data_dir + &#39;&#x2F;valid&#x2F;cats&#x2F;&#39;</span><br><span class="line">test_dogs_dir &#x3D; data_dir + &#39;&#x2F;valid&#x2F;dogs&#x2F;&#39;</span><br><span class="line"></span><br><span class="line">未完待续</span><br></pre></td></tr></table></figure>
      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-tensorflow2-0系列-八-：训练过程可视化" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/19/tensorflow2-0%E7%B3%BB%E5%88%97-%E5%85%AB-%EF%BC%9A%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96/"
    >tensorflow2.0系列(八)：训练过程可视化</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/19/tensorflow2-0%E7%B3%BB%E5%88%97-%E5%85%AB-%EF%BC%9A%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96/" class="article-date">
  <time datetime="2020-03-19T15:54:37.000Z" itemprop="datePublished">2020-03-19</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>有时，你希望查看模型训练过程中各个参数的变化情况(例如损失函数loss的值)。虽然可以通过命令行输出来查看，但有时显得不够直观。而TensorBoard就是一个能够帮助我们将训练过程可视化的工具。<br>实时查看参数变化情况:<br>首先在代码目录下建立一个文件夹(如./tensorboard)存放TensorBoard的记录文件，并在代码中实例化一个记录器：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary_writer &#x3D; tf.summary.create_file_writer(&#39;.&#x2F;tensorboard&#39;)     # 参数为记录文件所保存的目录</span><br></pre></td></tr></table></figure>

<p>接下来，当需要记录训练过程中的参数时，通过with语句指定希望使用的记录器，并对需要记录的参数(一般是scalar)运行tf.summary.scalar(name,tensor,step=batch_index),即可将训练过程中参数在step时候的值记录下来。这里的 step 参数可根据自己的需要自行制定，一般可设置为当前训练过程中的batch序号。整体框架如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">summary_writer &#x3D; tf.summary.create_file_writer(&#39;.&#x2F;tensorboard&#39;)</span><br><span class="line"># 开始模型训练</span><br><span class="line">for batch_index in range(num_batches):</span><br><span class="line">    # ...（训练代码，当前batch的损失值放入变量loss中）</span><br><span class="line">    with summary_writer.as_default():                               # 希望使用的记录器</span><br><span class="line">        tf.summary.scalar(&quot;loss&quot;, loss, step&#x3D;batch_index)</span><br><span class="line">        tf.summary.scalar(&quot;MyScalar&quot;, my_scalar, step&#x3D;batch_index)  # 还可以添加其他自定义的变量</span><br></pre></td></tr></table></figure>
<p>每运行一次tf.summary.scalar()，记录器就会向记录文件中写入一条记录。除了最简单的标量(scalar)以外，TensorBoard还可以对其他类型的数据(如图像，音频等)进行可视化，详见<a href="http://www.tensorfly.cn/tfdoc/how_tos/summaries_and_tensorboard.html" target="_blank" rel="noopener">TensorBoard文档</a>。<br>当我们要对训练过程可视化时，在代码目录打开终端（如需要的话进入TensorFlow的conda环境），运行:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir&#x3D;.&#x2F;tensorboard</span><br></pre></td></tr></table></figure>
<p>然后使用浏览器访问命令行程序所输出的网址（一般是http://计算机名称：6006），即可访问TensorBoard的可视界面。默认情况下，TensorBoard 每30秒更新一次数据。不过也可以点击右上角的刷新按钮手动刷新。TensorBoard的使用有以下注意事项：<br>[1]<br>记录文件夹目录保持全英文。</p>
<p>查看Graph和Profile:<br>除此以外，我们可以在训练时使用tf.summary.trace_on开启Trace，此时TensorFlow会将训练时的大量信息（如计算图的结构，每个操作所耗费的时间等）记录下来。在训练完成后，使用tf.summary.trace_export将记录结果输出到文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.trace_on(graph&#x3D;True, profiler&#x3D;True)  # 开启Trace，可以记录图结构和profile信息</span><br><span class="line"># 进行训练</span><br><span class="line">with summary_writer.as_default():</span><br><span class="line">    tf.summary.trace_export(name&#x3D;&quot;model_trace&quot;, step&#x3D;0, profiler_outdir&#x3D;log_dir)    # 保存Trace信息到文件</span><br></pre></td></tr></table></figure>
<p>之后，我们就可以在TensorBoard中选择“Profile”，以时间轴的方式查看各操作的耗时情况。如果使用了tf.function建立了计算图，也可以点击“Graphs”查看图结构。</p>
<p>可视化多层感知机模型的训练过程：</p>
<pre><code>import tensorflow as tf
from zh.model.mnist.mlp import MLP
from zh.model.utils import MNISTLoader

num_batches = 1000
batch_size = 50
learning_rate = 0.001
log_dir = &apos;tensorboard&apos;
model = MLP()
data_loader = MNISTLoader()
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
summary_writer = tf.summary.create_file_writer(log_dir)     # 实例化记录器
tf.summary.trace_on(profiler=True)  # 开启Trace（可选）
for batch_index in range(num_batches):
    X, y = data_loader.get_batch(batch_size)
    with tf.GradientTape() as tape:
        y_pred = model(X)
        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)
        loss = tf.reduce_mean(loss)
        print(&quot;batch %d: loss %f&quot; % (batch_index, loss.numpy()))
        with summary_writer.as_default():                           # 指定记录器
            tf.summary.scalar(&quot;loss&quot;, loss, step=batch_index)       # 将当前损失函数的值写入记录器
    grads = tape.gradient(loss, model.variables)
    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))
with summary_writer.as_default():
    tf.summary.trace_export(name=&quot;model_trace&quot;, step=0, profiler_outdir=log_dir)    # 保存Trace信息到文件（可选）</code></pre>
      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-tensorflow2-0系列-七-：变量的保存与恢复" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/19/tensorflow2-0%E7%B3%BB%E5%88%97-%E4%B8%83-%EF%BC%9A%E5%8F%98%E9%87%8F%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E6%81%A2%E5%A4%8D/"
    >tensorflow2.0系列(七)：变量的保存与恢复</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/19/tensorflow2-0%E7%B3%BB%E5%88%97-%E4%B8%83-%EF%BC%9A%E5%8F%98%E9%87%8F%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E6%81%A2%E5%A4%8D/" class="article-date">
  <time datetime="2020-03-19T09:12:05.000Z" itemprop="datePublished">2020-03-19</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>变量的保存与恢复<br>很多时候，我们希望在模型训练完成后能将训练好的参数（变量）保存起来。在需要使用模型的其他地方载入模型和参数，就能直接得到训练好的模型。你第一个想到的可能是用Python的序列化模块pickle来存储model.variables。但是这种方法并不能保存tensorflow生成的变量。<br>pickle模块：<br>通过python的pickle模块实现了对象的序列和反序列化。<br>通过pickle模块的序列化操作我们能够将程序中运行的对象信息保存到文件中去，永久存储。<br>通过pickle模块的反序列化操作，我们能够从文件中创建上一次程序保存的对象。<br>基本接口：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pickle.dump(obj, file, [,protocol])</span><br><span class="line">x &#x3D; pickle.load(file)</span><br></pre></td></tr></table></figure>
<p>这里的protocol有什么用。[1]</p>
<p>TensorFlow提供了tf.train.Checkpoint这一强大的变量保存与恢复类，使用save()和restore()方法即可将TensorFlow中所有包含Checkpointable State的对象进行保存和恢复。具体的说，tf.keras.optimizer、tf.Variable、tf.keras.Layer或者tf.keras.Model实例都可以被保存。<br>使用方法非常简单，例如，如果我们希望保存一个继承tf.keras.Model的模型实例model和一个继承tf.train.Optimizer的优化器optimizer，我们可以这样写：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">checkpoint &#x3D; tf.train.Checkpoint(myAwesomeModel&#x3D;model, myAwesomeOptimizer&#x3D;optimizer)</span><br></pre></td></tr></table></figure>
<p>tf.train.Checkpoint()接受的参数比较特殊，是一个**kwargs。具体而言，是一系列的键值对，键名可以随意取，值为需要保存的对象。myAwesomeModel是我们为待保存的模型model所取的任意键名。注意，在恢复变量的时候，我们还将使用这一键名。</p>
<p>接下来，当模型训练完成需要保存的时候，使用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">checkpoint.save(save_path_with_prefix)</span><br></pre></td></tr></table></figure>
<p>save_path_with_prefix是保存文件的目录+前缀。<br>在源代码目录建立一个名为save的文件夹并执行一次checkpoint.save(‘./save/model.ckpt’)，我们就可以在可以在save目录下发现名为checkpoint、model.ckpt-1.index、model.ckpt-1.data-00000-of-00001的三个文件，这些文件记录了变量信息。checkpoint.save()方法可以运行多次，每运行一次都会得到一个.index文件和.data文件，序号依次累加。</p>
<p>当在其他地方需要为模型重新载入之前保存的参数时，需要再次实例化一个checkpoint，同时保持键名的一致。再调用checkpoint的restore方法。如下面代码所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_to_be_restored &#x3D; MyModel()                                        # 待恢复参数的同一模型</span><br><span class="line">checkpoint &#x3D; tf.train.Checkpoint(myAwesomeModel&#x3D;model_to_be_restored)   # 键名保持为“myAwesomeModel”</span><br><span class="line">checkpoint.restore(save_path_with_prefix_and_index)</span><br></pre></td></tr></table></figure>
<p>即可恢复模型变量。save_path_with_prefix_and_index是之前保存的文件的目录+前缀+编号。例如，调用checkpoint.restore(‘./save/model.ckpt-1’)就可以载入前缀为model.ckpt，序号为1的文件来恢复模型。当保存了多个文件时，我们往往想载入最近的一个。可以使用tf.train.latest_checkpoint(save_path)这个辅助函数返回目录下最近一次checkpoint的文件名。例如如果save目录下有model.ckpt-1.index到model.ckpt-10.index的10个保存文件，tf.train.latest_checkpoint(‘./save’)即返回./save/model.ckpt-10。</p>
<p>总体而言，恢复与保存变量的典型代码框架如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># train.py 模型训练阶段</span><br><span class="line">model &#x3D; MyModel()</span><br><span class="line"># 实例化Checkpoint，指定保存对象为model（如果需要保存Optimizer的参数也可加入）</span><br><span class="line">checkpoint &#x3D; tf.train.Checkpoint(myModel&#x3D;model)</span><br><span class="line"># ...（模型训练代码）</span><br><span class="line"># 模型训练完毕后将参数保存到文件（也可以在模型训练过程中每隔一段时间就保存一次）</span><br><span class="line">checkpoint.save(&#39;.&#x2F;save&#x2F;model.ckpt&#39;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># test.py 模型使用阶段</span><br><span class="line">model &#x3D; MyModel()</span><br><span class="line">checkpoint &#x3D; tf.train.Checkpoint(myModel&#x3D;model)             # 实例化Checkpoint，指定恢复对象为model</span><br><span class="line">checkpoint.restore(tf.train.latest_checkpoint(&#39;.&#x2F;save&#39;))    # 从文件恢复模型参数</span><br><span class="line"># 模型使用代码</span><br></pre></td></tr></table></figure>

<p>[1]</p>
<p>以多层感知机模型为例展示模型变量的保存和载入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2]</span><br></pre></td></tr></table></figure>

<p>使用tf.train.CheckpointManager删除旧的Checkpoint以及自定义文件编号:<br>在模型的训练过程中，我们往往每隔一定步数保存一个Checkpoint并进行编号。不过很多时候我们会有这样的需求：<br>在长时间的训练后，程序会保存大量的 Checkpoint，但我们只想保留最后的几个 Checkpoint；<br>Checkpoint默认从1开始编号，每次累加1，但我们可能希望使用别的编号方式(例如使用当前Batch的编号作为文件编号)。<br>这时，我们可以使用TensorFlow的tf.train.CheckpointManager来实现以上需求。具体的说，就是在定义Checkpoint后接着定义一个CheckpointManager：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">checkpoint &#x3D; tf.train.Checkpoint(model&#x3D;model)</span><br><span class="line">manager &#x3D; tf.train.CheckpointManager(checkpoint, directory&#x3D;&#39;.&#x2F;save&#39;, checkpoint_name&#x3D;&#39;model.ckpt&#39;, max_to_keep&#x3D;k)</span><br></pre></td></tr></table></figure>
<p>此处，directory参数为文件保存的路径，checkpoint_name为文件名前缀(不提供则默认为ckpt)，max_to_keep为保留的Checkpoint数目。在需要保存模型的时候，我们直接使用manager.save()即可。如果我们希望指定保存的Checkpoint的编号，则可以在保存时传入checkpoint_number参数。例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">manager.save(checkpoint_number&#x3D;100)</span><br></pre></td></tr></table></figure>

<p>以下提供一个实例，展示使用CheckpointManager限制仅保留最后三个Checkpoint文件，并使用batch的编号作为Checkpoint的文件编号。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[3]</span><br></pre></td></tr></table></figure>
      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-tensorflow2-0系列-六-：灵活建立模型" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/19/tensorflow2-0%E7%B3%BB%E5%88%97-%E5%85%AD-%EF%BC%9A%E7%81%B5%E6%B4%BB%E5%BB%BA%E7%AB%8B%E6%A8%A1%E5%9E%8B/"
    >tensorflow2.0系列(六)：灵活建立模型</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/19/tensorflow2-0%E7%B3%BB%E5%88%97-%E5%85%AD-%EF%BC%9A%E7%81%B5%E6%B4%BB%E5%BB%BA%E7%AB%8B%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time datetime="2020-03-19T06:32:30.000Z" itemprop="datePublished">2020-03-19</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>两种非继承类建立模型的方法：<br>sequential和functional<br>sequential：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model &#x3D; tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(100, activation&#x3D;tf.nn.relu),</span><br><span class="line">    tf.keras.layers.Dense(10),</span><br><span class="line">    tf.keras.layers.Softmax()</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<p>functional：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">inputs &#x3D; tf.keras.Input(shape&#x3D;(28, 28, 1))</span><br><span class="line">x &#x3D; tf.keras.layers.Flatten()(inputs)</span><br><span class="line">x &#x3D; tf.keras.layers.Dense(units&#x3D;100, activation&#x3D;tf.nn.relu)(x)</span><br><span class="line">x &#x3D; tf.keras.layers.Dense(units&#x3D;10)(x)</span><br><span class="line">outputs &#x3D; tf.keras.layers.Softmax()(x)</span><br><span class="line">model &#x3D; tf.keras.Model(inputs&#x3D;inputs, outputs&#x3D;outputs)</span><br></pre></td></tr></table></figure>

<p>通过这两种方式建立的模型要通过tf.keras.Model的compile方法配置训练过程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer&#x3D;tf.keras.optimizers.Adam(learning_rate&#x3D;0.001),</span><br><span class="line">    loss&#x3D;tf.keras.losses.sparse_categorical_crossentropy,</span><br><span class="line">    metrics&#x3D;[tf.keras.metrics.sparse_categorical_accuracy]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>tf.keras.Model.compile接受3个重要的参数：<br>oplimizer：优化器，从tf.keras.optimizers中选择；loss：损失函数，从tf.keras.losses中选择；metrics：评估指标，从tf.keras.metrics中选择。</p>
<p>接下来，使用tf.keras.Model的fit方法训练模型：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(data_loader.train_data, data_loader.train_label, epochs&#x3D;num_epochs, batch_size&#x3D;batch_size)</span><br></pre></td></tr></table></figure>
<p>tf.keras.Model.fit接受5个重要的参数：x：训练数据；y：目标数据(数据标签)；epochs：将训练数据迭代多少遍；batch_size：批次的大小；validation_data：验证数据，可用于在训练过程中监控模型的性能。<br>最后，使用tf.keras.Model.evaluate评估训练效果，提供测试数据及标签即可：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(model.evaluate(data_loader.test_data, data_loader.test_label))</span><br></pre></td></tr></table></figure>

<p>自定义层：不会</p>
<p>自定义损失函数和评估指标：<br>自定义损失函数需要继承tf.keras.losses.Loss类，重写call方法即可，传入真实值y_true和模型预测值 y_pred，输出模型预测值和真实值之间通过自定义的损失函数计算出的损失值。<br>下面的示例为均方差损失函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class MeanSquaredError(tf.keras.losses.Loss):</span><br><span class="line">    def call(self, y_true, y_pred):</span><br><span class="line">        return tf.reduce_mean(tf.square(y_pred - y_true))</span><br></pre></td></tr></table></figure>

<p>自定义评估指标：不会</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-北师大图论系列-一" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/19/%E5%8C%97%E5%B8%88%E5%A4%A7%E5%9B%BE%E8%AE%BA%E7%B3%BB%E5%88%97-%E4%B8%80/"
    >北师大图论系列(一)</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/19/%E5%8C%97%E5%B8%88%E5%A4%A7%E5%9B%BE%E8%AE%BA%E7%B3%BB%E5%88%97-%E4%B8%80/" class="article-date">
  <time datetime="2020-03-19T03:30:05.000Z" itemprop="datePublished">2020-03-19</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p><img src="/images/tulun/p1.jpg" alt=""></p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tulun/" rel="tag">tulun</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-tensorflow2-0系列-五-：卷积神经网络" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/18/tensorflow2-0%E7%B3%BB%E5%88%97-%E4%BA%94-%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"
    >tensorflow2.0系列(五)：卷积神经网络</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/18/tensorflow2-0%E7%B3%BB%E5%88%97-%E4%BA%94-%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="article-date">
  <time datetime="2020-03-18T11:53:59.000Z" itemprop="datePublished">2020-03-18</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>卷积神经网络的搭建：<br>卷积神经网络实际上是对大脑视觉皮层的神经元进行的建模，灵感来自于视觉皮层神经元每次只与前一层某个区域内神经元相连的现象。<br>代码示范：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">class CNN(tf.keras.Model):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv1 &#x3D; tf.keras.layers.Conv2D(</span><br><span class="line">            filters&#x3D;32,             # 卷积层神经元（卷积核）数目</span><br><span class="line">            kernel_size&#x3D;[5, 5],     # 感受野大小</span><br><span class="line">            padding&#x3D;&#39;same&#39;,         # padding策略（vaild 或 same）</span><br><span class="line">            activation&#x3D;tf.nn.relu   # 激活函数</span><br><span class="line">        )</span><br><span class="line">        self.pool1 &#x3D; tf.keras.layers.MaxPool2D(pool_size&#x3D;[2, 2], strides&#x3D;2)</span><br><span class="line">        self.conv2 &#x3D; tf.keras.layers.Conv2D(</span><br><span class="line">            filters&#x3D;64,</span><br><span class="line">            kernel_size&#x3D;[5, 5],</span><br><span class="line">            padding&#x3D;&#39;same&#39;,</span><br><span class="line">            activation&#x3D;tf.nn.relu</span><br><span class="line">        )</span><br><span class="line">        self.pool2 &#x3D; tf.keras.layers.MaxPool2D(pool_size&#x3D;[2, 2], strides&#x3D;2)</span><br><span class="line">        self.flatten &#x3D; tf.keras.layers.Reshape(target_shape&#x3D;(7 * 7 * 64,))</span><br><span class="line">        self.dense1 &#x3D; tf.keras.layers.Dense(units&#x3D;1024, activation&#x3D;tf.nn.relu)</span><br><span class="line">        self.dense2 &#x3D; tf.keras.layers.Dense(units&#x3D;10)</span><br><span class="line"></span><br><span class="line">    def call(self, inputs):</span><br><span class="line">        x &#x3D; self.conv1(inputs)                  # [batch_size, 28, 28, 32]</span><br><span class="line">        x &#x3D; self.pool1(x)                       # [batch_size, 14, 14, 32]</span><br><span class="line">        x &#x3D; self.conv2(x)                       # [batch_size, 14, 14, 64]</span><br><span class="line">        x &#x3D; self.pool2(x)                       # [batch_size, 7, 7, 64]</span><br><span class="line">        x &#x3D; self.flatten(x)                     # [batch_size, 7 * 7 * 64]</span><br><span class="line">        x &#x3D; self.dense1(x)                      # [batch_size, 1024]</span><br><span class="line">        x &#x3D; self.dense2(x)                      # [batch_size, 10]</span><br><span class="line">        output &#x3D; tf.nn.softmax(x)</span><br><span class="line">        return output</span><br></pre></td></tr></table></figure>
<p>感受野大小该怎么去理解。[3]<br>将上一节的model = MLP()换成 model = CNN()，可以发现准确率得到了显著的提高。</p>
<p>tf.keras.applications 中有一些预定义好的经典卷积神经网络结构，比如VGG16、VGG19 、ResNet、MobileNet等。我们可以直接调用这些经典的卷积神经网络结构（甚至载入预训练的参数），而无需手动定义网络结构。<br>比如，我们可以使用下列代码实例化一个MobileNetV2网络：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model &#x3D; tf.keras.applications.MobileNetV2()</span><br></pre></td></tr></table></figure>
<p>这行代码就相当于自己按照MobileNetV2的网络结构手写了一个类并且实例化:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class MobileNetV2():</span><br><span class="line">	pass</span><br><span class="line">model &#x3D; MobileNetV2()</span><br></pre></td></tr></table></figure>
<p>每个网络结构具有自己特定的详细参数设置,一些共通的常用参数如下：<br>input_shape：输入张量的形状(不含第一维的Batch)，大多默认为224×224×3。一般而言，模型对输入张量的大小有下限，长和宽至少为32×32或75×75。include_top:在网络的最后是否包含全连接层，默认为True；weights ：预训练权值，默认为’imagenet’，即为当前模型载入在ImageNet数据集上预训练的权值。如需随机初始化变量可设为None；classes：分类数，默认为1000。修改该参数需要include_top参数为True且weights参数为None。<br>下面展示一个例子，使用 MobileNetV2网络在tf_flowers五分类数据集上训练。通过将 weights设置为None，我们随机初始化变量而不使用预训练权值。同时将classes设置为5，对应于5分类的数据集。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow_datasets as tfds</span><br><span class="line"></span><br><span class="line">num_batches &#x3D; 1000</span><br><span class="line">batch_size &#x3D; 50</span><br><span class="line">learning_rate &#x3D; 0.001</span><br><span class="line"></span><br><span class="line">dataset &#x3D; tfds.load(&quot;tf_flowers&quot;, split&#x3D;tfds.Split.TRAIN, as_supervised&#x3D;True)</span><br><span class="line">dataset &#x3D; dataset.map(lambda img, label: (tf.image.resize(img, [224, 224]) &#x2F; 255.0, label)).shuffle(1024).batch(32)</span><br><span class="line">model &#x3D; tf.keras.applications.MobileNetV2(weights&#x3D;None, classes&#x3D;5)</span><br><span class="line">optimizer &#x3D; tf.keras.optimizers.Adam(learning_rate&#x3D;learning_rate)</span><br><span class="line">for images, labels in dataset:</span><br><span class="line">    with tf.GradientTape() as tape:</span><br><span class="line">        labels_pred &#x3D; model(images)</span><br><span class="line">        loss &#x3D; tf.keras.losses.sparse_categorical_crossentropy(y_true&#x3D;labels, y_pred&#x3D;labels_pred)</span><br><span class="line">        loss &#x3D; tf.reduce_mean(loss)</span><br><span class="line">        print(&quot;loss %f&quot; % loss.numpy())</span><br><span class="line">    grads &#x3D; tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(grads_and_vars&#x3D;zip(grads, model.trainable_variables))</span><br></pre></td></tr></table></figure>
<p>有一个小的区别是model.variables与model.trainable_variables。<br>加载数据和预处理数据的那两行代码没看懂。[1]</p>
<p>用tensorflow验证手推的卷积层运算：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># TensorFlow 的图像表示为 [图像数目，长，宽，色彩通道数] 的四维张量</span><br><span class="line"># 这里我们的输入图像 image 的张量形状为 [1, 7, 7, 1]</span><br><span class="line">image &#x3D; np.array([[</span><br><span class="line">    [0, 0, 0, 0, 0, 0, 0],</span><br><span class="line">    [0, 1, 0, 1, 2, 1, 0],</span><br><span class="line">    [0, 0, 2, 2, 0, 1, 0],</span><br><span class="line">    [0, 1, 1, 0, 2, 1, 0],</span><br><span class="line">    [0, 0, 2, 1, 1, 0, 0],</span><br><span class="line">    [0, 2, 1, 1, 2, 0, 0],</span><br><span class="line">    [0, 0, 0, 0, 0, 0, 0]</span><br><span class="line">]], dtype&#x3D;np.float32)</span><br><span class="line">image &#x3D; np.expand_dims(image, axis&#x3D;-1)  </span><br><span class="line">W &#x3D; np.array([[</span><br><span class="line">    [ 0, 0, -1], </span><br><span class="line">    [ 0, 1, 0 ], </span><br><span class="line">    [-2, 0, 2 ]</span><br><span class="line">]], dtype&#x3D;np.float32)</span><br><span class="line">b &#x3D; np.array([1], dtype&#x3D;np.float32)</span><br><span class="line"></span><br><span class="line">model &#x3D; tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Conv2D(</span><br><span class="line">        filters&#x3D;1,              # 卷积层神经元（卷积核）数目</span><br><span class="line">        kernel_size&#x3D;[3, 3],     # 感受野大小</span><br><span class="line">        kernel_initializer&#x3D;tf.constant_initializer(W),</span><br><span class="line">        bias_initializer&#x3D;tf.constant_initializer(b)</span><br><span class="line">    )]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">output &#x3D; model(image)</span><br><span class="line">print(tf.squeeze(output))</span><br></pre></td></tr></table></figure>
<p>tf.squeeze()方法的作用很有意思<br>有一个疑惑的地方就是一个四维的张量通过卷积层的输出是什么。[2]</p>
<p>如果图像是彩色的(即三通道)，这时我们就要为每个通道准备一个3×3的权值矩阵，即一共有3×3×3=27个权值。对于每个通道，均使用自己的权值矩阵进行处理，输出时将多个通道所输出的值进行加和。<br>在上面的代码中，每次卷积的结果都会丢掉一部分大小，有时这会为后面的工作带来麻烦。我们可以通过将tf.keras.layers.Conv2D中的padding参数设为same，从而使输出的矩阵大小和输入一致。<br>我们使用的是滑动窗口的方式进行卷积，所以可以设置每次滑动的步长。通过tf.keras.layers.Conv2D的strides参数即可设置步长（默认为1）。池化层现在一般用的都是max pooling，average pooling已经很少用了。</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-Andrew课后作业系列（一）" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/17/Andrew%E8%AF%BE%E5%90%8E%E4%BD%9C%E4%B8%9A%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89/"
    >Andrew课后作业系列（一）</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/17/Andrew%E8%AF%BE%E5%90%8E%E4%BD%9C%E4%B8%9A%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89/" class="article-date">
  <time datetime="2020-03-17T12:46:07.000Z" itemprop="datePublished">2020-03-17</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>用python实现hello world</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test &#x3D; &quot;Hello World&quot;</span><br><span class="line">print (&quot;test: &quot; + test)</span><br></pre></td></tr></table></figure>

<p>用math实现sigmoid函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import math</span><br><span class="line">def basic_sigmoid(x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Compute sigmoid of x.</span><br><span class="line">    Arguments:</span><br><span class="line">    x -- A scalar</span><br><span class="line">    Return:</span><br><span class="line">    s -- sigmoid(x)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    s &#x3D; 1&#x2F;(1+math.exp(-x))</span><br><span class="line">    return s</span><br></pre></td></tr></table></figure>

<p>basic_sigmoid()函数测试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">### One reason why we use &quot;numpy&quot; instead of &quot;math&quot; in Deep Learning ###</span><br><span class="line">x &#x3D; [1, 2, 3]</span><br><span class="line">basic_sigmoid(x) # you will see this give an error when you run it, because x is a vector.</span><br></pre></td></tr></table></figure>

<p>使用numpy而不是math库的一个很重要的原因就是math库不能传入一个向量，而numpy可以。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"># example of np.exp</span><br><span class="line">x &#x3D; np.array([1, 2, 3])</span><br><span class="line">print(np.exp(x)) # result is (exp(1), exp(2), exp(3))</span><br></pre></td></tr></table></figure>

<p>在python中，向量同样可以与标量相加，因为python的广播机制</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># example of vector operation</span><br><span class="line">x &#x3D; np.array([1, 2, 3])</span><br><span class="line">print (x + 3)</span><br></pre></td></tr></table></figure>

<p>用numpy实现sigmoid函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: sigmoid</span><br><span class="line">import numpy as np # this means you can access numpy functions by writing np.function() instead of numpy.function()</span><br><span class="line">def sigmoid(x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Compute the sigmoid of x</span><br><span class="line">    Arguments:</span><br><span class="line">    x -- A scalar or numpy array of any size</span><br><span class="line">    Return:</span><br><span class="line">    s -- sigmoid(x)</span><br><span class="line">    &quot;&quot;&quot; </span><br><span class="line">    s &#x3D; 1&#x2F;(1+np.exp(-x)) </span><br><span class="line">    return s</span><br></pre></td></tr></table></figure>

<p>测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; np.array([1, 2, 3])</span><br><span class="line">sigmoid(x)</span><br></pre></td></tr></table></figure>

<p>用numpy实现sigmoid的导数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def sigmoid_derivative(x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.</span><br><span class="line">    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.</span><br><span class="line">    Arguments:</span><br><span class="line">    x -- A scalar or numpy array</span><br><span class="line">    Return:</span><br><span class="line">    ds -- Your computed gradient.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    s &#x3D; 1&#x2F;(1+np.exp(-x))</span><br><span class="line">    ds &#x3D; s*(1-s)    </span><br><span class="line">    return ds</span><br></pre></td></tr></table></figure>

<p>测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; np.array([1, 2, 3])</span><br><span class="line">print (&quot;sigmoid_derivative(x) &#x3D; &quot; + str(sigmoid_derivative(x)))</span><br></pre></td></tr></table></figure>

<p>shape()和reshape()的使用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def image2vector(image):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Argument:</span><br><span class="line">    image -- a numpy array of shape (length, height, depth)</span><br><span class="line">    Returns:</span><br><span class="line">    v -- a vector of shape (length*height*depth, 1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    v &#x3D; image.reshape(image.shape[0]*image.shape[1],image.shape[2])</span><br><span class="line">    return v</span><br></pre></td></tr></table></figure>

<p>测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values</span><br><span class="line">image &#x3D; np.array([[[ 0.67826139,  0.29380381],</span><br><span class="line">        [ 0.90714982,  0.52835647],</span><br><span class="line">        [ 0.4215251 ,  0.45017551]],</span><br><span class="line"></span><br><span class="line">       [[ 0.92814219,  0.96677647],</span><br><span class="line">        [ 0.85304703,  0.52351845],</span><br><span class="line">        [ 0.19981397,  0.27417313]],</span><br><span class="line"></span><br><span class="line">       [[ 0.60659855,  0.00533165],</span><br><span class="line">        [ 0.10820313,  0.49978937],</span><br><span class="line">        [ 0.34144279,  0.94630077]]])</span><br><span class="line"></span><br><span class="line">print (&quot;image2vector(image) &#x3D; &quot; + str(image2vector(image)))</span><br></pre></td></tr></table></figure>

<p>按行归一化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def normalizeRows(x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement a function that normalizes each row of the matrix x (to have unit length). </span><br><span class="line">    Argument:</span><br><span class="line">    x -- A numpy matrix of shape (n, m)</span><br><span class="line">    Returns:</span><br><span class="line">    x -- The normalized (by row) numpy matrix. You are allowed to modify x.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord &#x3D; 2, axis &#x3D; ..., keepdims &#x3D; True)</span><br><span class="line">    x_norm &#x3D; np.linalg.norm(x,ord&#x3D;2,axis&#x3D;1,keepdims&#x3D;True) </span><br><span class="line">    # Divide x by its norm.</span><br><span class="line">    x &#x3D; x&#x2F;x_norm</span><br><span class="line">    return x</span><br></pre></td></tr></table></figure>
<p>np.linalg.norm()的使用还要查一下。[1]</p>
<p>测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; np.array([</span><br><span class="line">    [0, 3, 4],</span><br><span class="line">    [1, 6, 4]])</span><br><span class="line">print(&quot;normalizeRows(x) &#x3D; &quot; + str(normalizeRows(x)))</span><br></pre></td></tr></table></figure>

<p>定义softmax函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def softmax(x):</span><br><span class="line">    &quot;&quot;&quot;Calculates the softmax for each row of the input x.</span><br><span class="line">    Your code should work for a row vector and also for matrices of shape (n, m).</span><br><span class="line">    Argument:</span><br><span class="line">    x -- A numpy matrix of shape (n,m)</span><br><span class="line">    Returns:</span><br><span class="line">    s -- A numpy matrix equal to the softmax of x, of shape (n,m)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # Apply exp() element-wise to x. Use np.exp(...).</span><br><span class="line">    x_exp &#x3D; np.exp(x)</span><br><span class="line">    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis &#x3D; 1, keepdims &#x3D; True).</span><br><span class="line">    x_sum &#x3D; np.sum(x_exp,axis&#x3D;1,keepdims&#x3D;True)</span><br><span class="line">    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.</span><br><span class="line">    s &#x3D; x_exp&#x2F;x_sum</span><br><span class="line">    return s</span><br></pre></td></tr></table></figure>
<p>np.sum()中的axis和keepdim两个参数是啥意思？[2]</p>
<p>测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; np.array([</span><br><span class="line">    [9, 2, 5, 0, 0],</span><br><span class="line">    [7, 5, 0, 0 ,0]])</span><br><span class="line">print(&quot;softmax(x) &#x3D; &quot; + str(softmax(x)))</span><br></pre></td></tr></table></figure>

<p>向量化计算与for循环在时间上的比较：<br>用for循环实现向量之间的运算</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">x1 &#x3D; [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]</span><br><span class="line">x2 &#x3D; [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]</span><br><span class="line">### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###</span><br><span class="line">tic &#x3D; time.process_time()</span><br><span class="line">dot &#x3D; 0</span><br><span class="line">for i in range(len(x1)):</span><br><span class="line">    dot+&#x3D; x1[i]*x2[i]</span><br><span class="line">toc &#x3D; time.process_time()</span><br><span class="line">print (&quot;dot &#x3D; &quot; + str(dot) + &quot;\n ----- Computation time &#x3D; &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br><span class="line">### CLASSIC OUTER PRODUCT IMPLEMENTATION ###</span><br><span class="line">tic &#x3D; time.process_time()</span><br><span class="line">outer &#x3D; np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros</span><br><span class="line">for i in range(len(x1)):</span><br><span class="line">    for j in range(len(x2)):</span><br><span class="line">        outer[i,j] &#x3D; x1[i]*x2[j]</span><br><span class="line">toc &#x3D; time.process_time()</span><br><span class="line">print (&quot;outer &#x3D; &quot; + str(outer) + &quot;\n ----- Computation time &#x3D; &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br><span class="line">### CLASSIC ELEMENTWISE IMPLEMENTATION ###</span><br><span class="line">tic &#x3D; time.process_time()</span><br><span class="line">mul &#x3D; np.zeros(len(x1))</span><br><span class="line">for i in range(len(x1)):</span><br><span class="line">    mul[i] &#x3D; x1[i]*x2[i]</span><br><span class="line">toc &#x3D; time.process_time()</span><br><span class="line">print (&quot;elementwise multiplication &#x3D; &quot; + str(mul) + &quot;\n ----- Computation time &#x3D; &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br><span class="line">### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###</span><br><span class="line">W &#x3D; np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array</span><br><span class="line">tic &#x3D; time.process_time()</span><br><span class="line">gdot &#x3D; np.zeros(W.shape[0])</span><br><span class="line">for i in range(W.shape[0]):</span><br><span class="line">    for j in range(len(x1)):</span><br><span class="line">        gdot[i] +&#x3D; W[i,j]*x1[j]</span><br><span class="line">toc &#x3D; time.process_time()</span><br><span class="line">print (&quot;gdot &#x3D; &quot; + str(gdot) + &quot;\n ----- Computation time &#x3D; &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br></pre></td></tr></table></figure>

<p>用numpy库直接进行向量计算：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">x1 &#x3D; [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]</span><br><span class="line">x2 &#x3D; [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]</span><br><span class="line">### VECTORIZED DOT PRODUCT OF VECTORS ###</span><br><span class="line">tic &#x3D; time.process_time()</span><br><span class="line">dot &#x3D; np.dot(x1,x2)</span><br><span class="line">toc &#x3D; time.process_time()</span><br><span class="line">print (&quot;dot &#x3D; &quot; + str(dot) + &quot;\n ----- Computation time &#x3D; &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br><span class="line">### VECTORIZED OUTER PRODUCT ###</span><br><span class="line">tic &#x3D; time.process_time()</span><br><span class="line">outer &#x3D; np.outer(x1,x2)</span><br><span class="line">toc &#x3D; time.process_time()</span><br><span class="line">print (&quot;outer &#x3D; &quot; + str(outer) + &quot;\n ----- Computation time &#x3D; &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br><span class="line">### VECTORIZED ELEMENTWISE MULTIPLICATION ###</span><br><span class="line">tic &#x3D; time.process_time()</span><br><span class="line">mul &#x3D; np.multiply(x1,x2)</span><br><span class="line">toc &#x3D; time.process_time()</span><br><span class="line">print (&quot;elementwise multiplication &#x3D; &quot; + str(mul) + &quot;\n ----- Computation time &#x3D; &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br><span class="line">### VECTORIZED GENERAL DOT PRODUCT ###</span><br><span class="line">tic &#x3D; time.process_time()</span><br><span class="line">dot &#x3D; np.dot(W,x1)</span><br><span class="line">toc &#x3D; time.process_time()</span><br><span class="line">print (&quot;gdot &#x3D; &quot; + str(dot) + &quot;\n ----- Computation time &#x3D; &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br></pre></td></tr></table></figure>

<p>用L1范数作为损失函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def L1(yhat, y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments:</span><br><span class="line">    yhat -- vector of size m (predicted labels)</span><br><span class="line">    y -- vector of size m (true labels)</span><br><span class="line">    Returns:</span><br><span class="line">    loss -- the value of the L1 loss function defined above</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    loss &#x3D; np.sum(abs(y-yhat))</span><br><span class="line">    return loss</span><br></pre></td></tr></table></figure>

<p>测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yhat &#x3D; np.array([.9, 0.2, 0.1, .4, .9])</span><br><span class="line">y &#x3D; np.array([1, 0, 0, 1, 1])</span><br><span class="line">print(&quot;L1 &#x3D; &quot; + str(L1(yhat,y)))</span><br></pre></td></tr></table></figure>

<p>用L2范数作为损失函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def L2(yhat, y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments:</span><br><span class="line">    yhat -- vector of size m (predicted labels)</span><br><span class="line">    y -- vector of size m (true labels)</span><br><span class="line">    Returns:</span><br><span class="line">    loss -- the value of the L2 loss function defined above</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    loss &#x3D; np.sum(np.dot(y-yhat,y-yhat))</span><br><span class="line">    return loss</span><br></pre></td></tr></table></figure>

<p>测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yhat &#x3D; np.array([.9, 0.2, 0.1, .4, .9])</span><br><span class="line">y &#x3D; np.array([1, 0, 0, 1, 1])</span><br><span class="line">print(&quot;L2 &#x3D; &quot; + str(L2(yhat,y)))</span><br></pre></td></tr></table></figure>
      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Andrew/" rel="tag">Andrew</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-tensorflow2-0系列-四-：多层感知机" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/17/tensorflow2-0%E7%B3%BB%E5%88%97-%E5%9B%9B-%EF%BC%9A%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"
    >tensorflow2.0系列(四)：多层感知机</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/17/tensorflow2-0%E7%B3%BB%E5%88%97-%E5%9B%9B-%EF%BC%9A%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/" class="article-date">
  <time datetime="2020-03-17T10:07:51.000Z" itemprop="datePublished">2020-03-17</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>多层感知机就是多层全连接神经网络。<br>在这里使用多层感知机完成 MNIST 手写体数字图片数据集 [LeCun1998] 的分类任务。<br>我们将依次完成以下几个环节。<br>1、使用 tf.keras.datasets 获得数据集并预处理<br>2、使用 tf.keras.Model 和 tf.keras.layers 构建模型<br>3、构建模型训练流程，使用 tf.keras.losses 计算损失函数，并使用 tf.keras.optimizer 优化模型<br>4、构建模型评估流程，使用 tf.keras.metrics 计算评估指标<br>第一步，先定义一个MNISTLoader类来下载MNIST数据集，预处理以及自定义一个从数据集中取出数据的方法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class MNISTLoader():</span><br><span class="line">    def __init__(self):</span><br><span class="line">        mnist &#x3D; tf.keras.datasets.mnist</span><br><span class="line">        (self.train_data, self.train_label), (self.test_data, self.test_label) &#x3D; mnist.load_data()</span><br><span class="line">        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道</span><br><span class="line">        self.train_data &#x3D; np.expand_dims(self.train_data.astype(np.float32) &#x2F; 255.0, axis&#x3D;-1)      # [60000, 28, 28, 1]</span><br><span class="line">        self.test_data &#x3D; np.expand_dims(self.test_data.astype(np.float32) &#x2F; 255.0, axis&#x3D;-1)        # [10000, 28, 28, 1]</span><br><span class="line">        self.train_label &#x3D; self.train_label.astype(np.int32)    # [60000]</span><br><span class="line">        self.test_label &#x3D; self.test_label.astype(np.int32)      # [10000]</span><br><span class="line">        self.num_train_data, self.num_test_data &#x3D; self.train_data.shape[0], self.test_data.shape[0]</span><br><span class="line"></span><br><span class="line">    def get_batch(self, batch_size):</span><br><span class="line">        # 从数据集中随机取出batch_size个元素并返回</span><br><span class="line">        index &#x3D; np.random.randint(0, np.shape(self.train_data)[0], batch_size)</span><br><span class="line">        return self.train_data[index, :], self.train_label[index]</span><br></pre></td></tr></table></figure>
<p>需要注意的是keras内置的load_data()方法返回的是两个元组以及get_batch()方法是怎么写的。</p>
<p>在 TensorFlow 中，图像数据集一般表示成一个[图像数目，长，宽，色彩通道数] 的四维张量。这里读入的是灰度图片，色彩通道数为1(彩色RGB图像色彩通道数为3)。由于load_data()方法返回的都是[图像数目，长，宽]的三维张量，所以使用np.expand_dims()函数给返回张量在最后添加一维，保持一致性。<br>多层感知机的模型与之前实现的线性模型不同的地方在于层数增加了，以及引入了非线性激活函数ReLU函数。该模型输入一个向量，输出一个10维的向量，每一维分别是识别为0-9的概率。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class MLP(tf.keras.Model):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.flatten &#x3D; tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平</span><br><span class="line">        self.dense1 &#x3D; tf.keras.layers.Dense(units&#x3D;100, activation&#x3D;tf.nn.relu)</span><br><span class="line">        self.dense2 &#x3D; tf.keras.layers.Dense(units&#x3D;10)</span><br><span class="line"></span><br><span class="line">    def call(self, inputs):         # [batch_size, 28, 28, 1]</span><br><span class="line">        x &#x3D; self.flatten(inputs)    # [batch_size, 784]</span><br><span class="line">        x &#x3D; self.dense1(x)          # [batch_size, 100]</span><br><span class="line">        x &#x3D; self.dense2(x)          # [batch_size, 10]</span><br><span class="line">        output &#x3D; tf.nn.softmax(x)</span><br><span class="line">        return output</span><br></pre></td></tr></table></figure>
<p>因为我们希望输出的向量具有这样两个特点：该向量中的每个元素均在[0,1]之间；该向量的所有元素之和为1。因此我们对最后一层的线性输出使用Softmax函数进行归一化。softmax不仅取出了向量中的最大值，还保留了对其余类别的预测信息，因此被称作soft argmax。<br>归一化就是把数值映射到(0,1)之间，有很多种方法。</p>
<p>第三步，就是构建模型训练流程，首先需要定义一些超参数，比如训练的轮数、一次喂入多少个数据，即batch_size、还有学习率。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num_epochs &#x3D; 5</span><br><span class="line">batch_size &#x3D; 50</span><br><span class="line">learning_rate &#x3D; 0.001</span><br></pre></td></tr></table></figure>
<p>接着将前两步定义的两个类实例化，并且实例化出一个优化器，这里采用的Adam优化器。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model &#x3D; MLP()</span><br><span class="line">data_loader &#x3D; MNISTLoader()</span><br><span class="line">optimizer &#x3D; tf.keras.optimizers.Adam(learning_rate&#x3D;learning_rate)</span><br></pre></td></tr></table></figure>
<p>接下来就是迭代进行一下步骤：<br>从 DataLoader 中随机取一批训练数据；<br>将这批数据送入模型，计算出模型的预测值；<br>将模型预测值与真实值进行比较，计算损失函数(loss)。这里使用tf.keras.losses中的交叉熵函数作为损失函数；<br>计算损失函数关于模型变量的导数；<br>将求出的导数值传入优化器，使用优化器的 apply_gradients 方法更新模型参数以最小化损失函数。<br>具体代码为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">num_batches &#x3D; int(data_loader.num_train_data &#x2F;&#x2F; batch_size * num_epochs)</span><br><span class="line">for batch_index in range(num_batches):</span><br><span class="line">    X, y &#x3D; data_loader.get_batch(batch_size)</span><br><span class="line">    with tf.GradientTape() as tape:</span><br><span class="line">        y_pred &#x3D; model(X)</span><br><span class="line">        loss &#x3D; tf.keras.losses.sparse_categorical_crossentropy(y_true&#x3D;y, y_pred&#x3D;y_pred)</span><br><span class="line">        loss &#x3D; tf.reduce_mean(loss)</span><br><span class="line">        print(&quot;batch %d: loss %f&quot; % (batch_index, loss.numpy()))</span><br><span class="line">    grads &#x3D; tape.gradient(loss, model.variables)</span><br><span class="line">    optimizer.apply_gradients(grads_and_vars&#x3D;zip(grads, model.variables))</span><br></pre></td></tr></table></figure>
<p>在这里，我们没有显式地写出一个损失函数，而是使用了tf.keras.losses中的sparse_categorical_crossentropy(交叉熵)函数，将模型的预测值y_pred与真实的标签值y作为函数参数传入，由Keras帮助我们计算损失函数的值。<br>交叉熵作为损失函数，在分类问题中被广泛应用。<br>第四步，评估模型。keras在metrics模块中封装了不同的评估器用于适应不同的需求。在这里我们使用的是SparseCategoricalAccuracy评估器来评估模型在测试集上的性能，该评估器输出的是预测正确的样本数占总样本数的比例。<br>在每次迭代测试集的时候，都会通过update_state()方法向评估器输入两个参数：y_pred和y_true，即模型预测出的结果和真实结果。评估器定义了内部变量来保存当前评估指标相关的参数数值(例如当前已传入的累计样本数和当前预测正确的样本数)。迭代结束后，我们使用result()方法输出最终的评估指标值(预测正确的样本数占总样本数的比例)。<br>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sparse_categorical_accuracy &#x3D; tf.keras.metrics.SparseCategoricalAccuracy()</span><br><span class="line">num_batches &#x3D; int(data_loader.num_test_data &#x2F;&#x2F; batch_size)</span><br><span class="line">for batch_index in range(num_batches):</span><br><span class="line">    start_index, end_index &#x3D; batch_index * batch_size, (batch_index + 1) * batch_size</span><br><span class="line">    y_pred &#x3D; model.predict(data_loader.test_data[start_index: end_index])</span><br><span class="line">    sparse_categorical_accuracy.update_state(y_true&#x3D;data_loader.test_label[start_index: end_index], y_pred&#x3D;y_pred)</span><br><span class="line">print(&quot;test accuracy: %f&quot; % sparse_categorical_accuracy.result())</span><br></pre></td></tr></table></figure>
<p>在这里我们首先实例化了一个 tf.keras.metrics.SparseCategoricalAccuracy 评估器，并且使用For循环迭代分批次传入了测试集数据的预测结果与真实结果，并调用result()方法输出了训练后的模型在测试数据集上的准确率。<br>但是同样可以看到keras的缺点就是啥都帮你做了你就很难知道背后的机理是怎么实现的了。</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-tensorflow2-0系列-三" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/17/tensorflow2-0%E7%B3%BB%E5%88%97-%E4%B8%89/"
    >tensorflow2.0系列(三)</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/17/tensorflow2-0%E7%B3%BB%E5%88%97-%E4%B8%89/" class="article-date">
  <time datetime="2020-03-17T04:58:42.000Z" itemprop="datePublished">2020-03-17</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>在keras中，通过继承tf.keras.Model来定义自己的模型。在继承类中，需要重写”<del><strong>init</strong></del>“()和和call(input)两个方法，也可以根据需要增加自定义的方法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class MyModel(tf.keras.Model):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()     # Python 2 下使用 super(MyModel, self).__init__()</span><br><span class="line">        &#x2F;&#x2F; 此处添加初始化代码（包含 call 方法中会用到的层），例如</span><br><span class="line">        &#x2F;&#x2F; layer1 &#x3D; tf.keras.layers.BuiltInLayer(...)</span><br><span class="line">        &#x2F;&#x2F; layer2 &#x3D; MyCustomLayer(...)</span><br><span class="line"></span><br><span class="line">    def call(self, input):</span><br><span class="line">        &#x2F;&#x2F; 此处添加模型调用的代码（处理输入并返回输出），例如</span><br><span class="line">        &#x2F;&#x2F; x &#x3D; layer1(input)</span><br><span class="line">        &#x2F;&#x2F; output &#x3D; layer2(x)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 还可以添加自定义的方法</span><br></pre></td></tr></table></figure>
<p>继承 tf.keras.Model 后，就以使用父类的方法和属性，比如在实例化类model = Model()后，就可以通过model.variables这一属性直接获得模型中的所有变量，而不需要我们一个个显式地指定变量。<br>用keras实现线性回归：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">X &#x3D; tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])</span><br><span class="line">y &#x3D; tf.constant([[10.0], [20.0]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Linear(tf.keras.Model):</span><br><span class="line">    def &quot;&#39;__init__&#39;&quot;(self):</span><br><span class="line">        super().&quot;&#39;__init__&#39;&quot;()</span><br><span class="line">        self.dense &#x3D; tf.keras.layers.Dense(</span><br><span class="line">            units&#x3D;1,</span><br><span class="line">            activation&#x3D;None,</span><br><span class="line">            kernel_initializer&#x3D;tf.zeros_initializer(),</span><br><span class="line">            bias_initializer&#x3D;tf.zeros_initializer()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def call(self, input):</span><br><span class="line">        output &#x3D; self.dense(input)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 以下代码结构与前节类似</span><br><span class="line">model &#x3D; Linear()</span><br><span class="line">optimizer &#x3D; tf.keras.optimizers.SGD(learning_rate&#x3D;0.01)</span><br><span class="line">for i in range(100):</span><br><span class="line">    with tf.GradientTape() as tape:</span><br><span class="line">        y_pred &#x3D; model(X)      # 调用模型 y_pred &#x3D; model(X) 而不是显式写出 y_pred &#x3D; a * X + b</span><br><span class="line">        loss &#x3D; tf.reduce_mean(tf.square(y_pred - y))</span><br><span class="line">    grads &#x3D; tape.gradient(loss, model.variables)    # 使用 model.variables 这一属性直接获得模型中的所有变量</span><br><span class="line">    optimizer.apply_gradients(grads_and_vars&#x3D;zip(grads, model.variables))</span><br><span class="line">print(model.variables)</span><br></pre></td></tr></table></figure>
<p>tf.keras.layers.Dense类的作用是实例化生成全连接层，它的参数有：<br>units ：输出张量的维度<br>activation ：激活函数，对应于f(AW + b)中的f，默认为无激活函数(a(x) = x)。常用的激活函数包括 tf.nn.relu、tf.nn.tanh和tf.nn.sigmoid；<br>use_bias ：是否加入偏置向量bias，即f(AW + b)中的b。默认为True；<br>kernel_initializer、bias_initializer：权重矩阵kernel和偏置向量bias两个变量的初始化器。默认为tf.glorot_uniform_initializer。设置为tf.zeros_initializer表示将两个变量均初始化为全0；<br>该层包含权重矩阵kernel=[input_dim,units]和偏置向量bias=[units]两个可训练变量，对应于f(AW+b)中的W和b。<br>tf.matmul(input, kernel) 的结果是一个形状为 [batch_size, units] 的二维矩阵，而bias是一个形状为[units]的一维向量，它们是怎么相加的呢？<br>这里用到的是python的广播机制。<br>为什么模型类是重写call()方法而不是”‘<strong>call</strong>‘“()方法？<br>实例化也可以理解为调用这个类。<br>在python中，myClass()==myClass.”‘<strong>call</strong>‘“()。重写call()方法而不是”‘<strong>call</strong>‘“()方法是因为在keras中call()方法是在”‘<strong>call</strong>‘“()方法中被调用的，也就是说，在keras框架下，在调用某个模型时，keras要执行的不仅仅是用户定义的call()方法，还有一些每个模型在调用时都要进行的处理，而这些东西就写在了””<strong>call</strong>‘“()方法下。因此用户重写的只能是call()方法。</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-白板推导系列-一-：SVM定义" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/16/%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97-%E4%B8%80-%EF%BC%9ASVM%E5%AE%9A%E4%B9%89/"
    >白板推导系列(一)：SVM定义</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/16/%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%E7%B3%BB%E5%88%97-%E4%B8%80-%EF%BC%9ASVM%E5%AE%9A%E4%B9%89/" class="article-date">
  <time datetime="2020-03-16T10:12:51.000Z" itemprop="datePublished">2020-03-16</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p><img src="/images/SVMdefinition.jpg" alt=""></p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      

    </footer>

  </div>

  

  
  
  

  

</article>
    
  </article>
  

  
  <nav class="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2015-2020
        John Doe
      </li>
      <li>
        
        Powered by
        
        
        <a href="https://hexo.io" target="_blank">Hexo</a> Theme <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <span>
  <i>PV:<span id="busuanzi_value_page_pv"></span></i>
  <i>UV:<span id="busuanzi_value_site_uv"></span></i>
</span>
        
      </li>
      
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
    <aside class="sidebar">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="畅院士的开山大弟子的博客"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E6%97%85%E8%A1%8C/">旅行</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="http://shenyu-vip.lofter.com" target="_blank" rel="noopener">摄影</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2019/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/share.js"></script>


<script src="/js/lazyload.min.js"></script>


<script>
  try {
    var typed = new Typed("#subtitle", {
      strings: ['面朝大海，春暖花开', '愿你一生努力，一生被爱', '想要的都拥有，得不到的都释怀'],
      startDelay: 0,
      typeSpeed: 200,
      loop: true,
      backSpeed: 100,
      showCursor: true
    });
  } catch (err) {
  }

</script>





<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/js/ayer.js"></script>



<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>




<script src="/js/busuanzi-2.3.pure.min.js"></script>



<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.2/dist/jquery.fancybox.min.css">
<script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.2/dist/jquery.fancybox.min.js"></script>


    
  </div>
</body>

</html>